I"<p>오늘은 KoNLPy, nltk, Keras를 이용해서 한국어 영화 리뷰의 감정을 분석하는 방법을 알아보겠습니다.</p>

<p>이진 분류 문제 중에서 대표적인 Keras에서 제공하는 imdb 데이터를 이용해서 긍정 부정을 예측하는 문제(<a href="http://nbviewer.jupyter.org/github/cyc1am3n/Deep-Learning-with-Python/blob/master/Chap03-getting_started_with_neural_networks/Chap03-4-classifying_movie_reviews.ipynb">링크</a>)와 거의 같은 과정을 거쳐 진행이 됩니다.</p>

<p>이 포스트에서 사용한 코드는 (<a href="http://nbviewer.jupyter.org/github/cyc1am3n/Deep-Learning-with-Python/blob/master/Chap03-getting_started_with_neural_networks/Chap03-Extra-classifying_korean_movie_review.ipynb">여기</a>) 에서도 확인할 수 있습니다.</p>

<hr />

<p><br /></p>

<h2 id="데이터셋-소개">데이터셋 소개</h2>

<hr />

<p>데이터셋은 <a href="http://github.com/e9t/nsmc/">Naver sentiment movie corpus</a>를 사용했습니다.</p>

<p>이 데이터셋은 네이버 영화의 리뷰 중 영화당 100개의 리뷰를 모아 총 200,000개의 리뷰(train: 15만, test: 5만)로 이루어져있고,
1~10점까지의 평점 중에서 중립적인 평점(5~8점)은 제외하고 1~4점을 긍정으로, 9~10점을 부정으로 동일한 비율로 데이터에 포함시켰습니다.</p>

<p><img src="/assets/img/posts/ml-with-python/nsmc_dataset.png" alt="" class="center-75" /><br />
<span class="caption"></span></p>

<p>데이터는 id, document, label 세 개의 열로 이루어져있습니다.
id는 리뷰의 고유한 key 값이고, document는 리뷰의 내용, label은 긍정(0)인지 부정(1)인지를 나타냅니다.
txt로 저장된 데이터를 처리하기 알맞게 list 형식으로 받아서 사용하겠습니다.</p>

<p><br /></p>

<h2 id="데이터-불러오기">데이터 불러오기</h2>

<hr />

<p>이제 nsmc 데이터가 어떤지 살펴보고 불러오겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cat</span> <span class="n">ratings_train</span><span class="p">.</span><span class="n">txt</span> <span class="o">|</span> <span class="n">head</span> <span class="o">-</span><span class="n">n</span> <span class="mi">10</span>

<span class="s">'''
OUTPUT:
id	document	label
9976970	아 더빙.. 진짜 짜증나네요 목소리	0
3819312	흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나	1
10265843	너무재밓었다그래서보는것을추천한다	0
9045019	교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정	0
6483659	사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다	1
5403919	막 걸음마 뗀 3세부터 초등학교 1학년생인 8살용영화.ㅋㅋㅋ...별반개도 아까움.	0
7797314	원작의 긴장감을 제대로 살려내지못했다.	0
9443947	별 반개도 아깝다 욕나온다 이응경 길용우 연기생활이몇년인지..정말 발로해도 그것보단 낫겟다 납치.감금만반복반복..이드라마는 가족도없다 연기못하는사람만모엿네	0
7156791	액션이 없는데도 재미 있는 몇안되는 영화	1
'''</span>
</code></pre></div></div>

<p><br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">read_data</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="s">'r'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="n">line</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="s">'</span><span class="se">\t</span><span class="s">'</span><span class="p">)</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">.</span><span class="n">read</span><span class="p">().</span><span class="n">splitlines</span><span class="p">()]</span>
        <span class="c1"># txt 파일의 헤더(id document label)는 제외하기
</span>        <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
    <span class="k">return</span> <span class="n">data</span>

<span class="n">train_data</span> <span class="o">=</span> <span class="n">read_data</span><span class="p">(</span><span class="s">'ratings_train.txt'</span><span class="p">)</span>
<span class="n">test_data</span> <span class="o">=</span> <span class="n">read_data</span><span class="p">(</span><span class="s">'ratings_test.txt'</span><span class="p">)</span>
</code></pre></div></div>

<p>데이터가 제대로 불러와졌는지 확인해봅시다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">train_data</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">test_data</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">test_data</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>

<span class="s">'''
OUTPUT:
150000
3
50000
3
'''</span>
</code></pre></div></div>

<p><br /></p>

<h2 id="데이터-전처리">데이터 전처리</h2>

<hr />

<p>이제 데이터를 학습하기에 알맞게 처리를 해볼텐데요, <strong>KoNLPy 라이브러리</strong>를 이용해서 형태소 분석 및 품사 태깅을 하겠습니다.</p>

<p>imdb 리뷰 분석 예제처럼 주어진 단어의 빈도만을 사용해서 처리해도 되지만 한국어는 영어와는 달리 띄어쓰기로 의미를 구분짓기에는 한계가 있고,</p>

<p>네이버 영화 데이터에는 맞춤법이나 띄어쓰기가 제대로 되어있지 않은 경우가 있기 때문에 정확한 분류를 위해서 KoNLPy를 이용하겠습니다.</p>

<p>KoNLPy는 띄어쓰기 알고리즘과 정규화를 이용해서 맞춤법이 틀린 문장도 어느 정도 고쳐주면서 형태소 분석과 품사를 태깅해주는 여러 클래스를 제공합니다. (<a href="https://konlpy-ko.readthedocs.io/ko/v0.4.3/morph/">링크</a> 참조)</p>

<p>그 중에서 <strong>Okt(Open Korean Text)</strong> 클래스를 이용하겠습니다.</p>

<p>먼저 Okt를 이용해서 간단한 문장을 분석해보겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">konlpy.tag</span> <span class="kn">import</span> <span class="n">Okt</span>

<span class="n">okt</span> <span class="o">=</span> <span class="n">Okt</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">okt</span><span class="p">.</span><span class="n">pos</span><span class="p">(</span><span class="sa">u</span><span class="s">'이 밤 그날의 반딧불을 당신의 창 가까이 보낼게요'</span><span class="p">))</span>

<span class="s">'''
OUTPUT:
[('이', 'Noun'), ('밤', 'Noun'), ('그날', 'Noun'), ('의', 'Josa'), ('반딧불', 'Noun'), ('을', 'Josa'), ('당신', 'Noun'), ('의', 'Josa'), ('창', 'Noun'), ('가까이', 'Noun'), ('보낼게요', 'Verb')]
'''</span>
</code></pre></div></div>

<p>이제 아까 불러온 데이터에 형태소 분석을 통해서 품사를 태깅해주는 작업을 하겠습니다.</p>

<p>데이터의 양이 큰 만큼 시간이 오래 걸리기 때문에 이 작업을 반복하지 않도록 한 번 태깅을 마친 후에는 json 파일로 저장하는 것을 추천합니다.</p>

<p>여기에서는 이미 태깅이 완료된 <code class="language-plaintext highlighter-rouge">train_docs.json</code> 파일이 존재하면 반복하지 않도록 만들었습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">pprint</span> <span class="kn">import</span> <span class="n">pprint</span>

<span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span><span class="n">doc</span><span class="p">):</span>
    <span class="c1"># norm은 정규화, stem은 근어로 표시하기를 나타냄
</span>    <span class="k">return</span> <span class="p">[</span><span class="s">'/'</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">okt</span><span class="p">.</span><span class="n">pos</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">stem</span><span class="o">=</span><span class="bp">True</span><span class="p">)]</span>

<span class="k">if</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">isfile</span><span class="p">(</span><span class="s">'train_docs.json'</span><span class="p">):</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">'train_docs.json'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">train_docs</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">'test_docs.json'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">test_docs</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">train_docs</span> <span class="o">=</span> <span class="p">[(</span><span class="n">tokenize</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">row</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span> <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">train_data</span><span class="p">]</span>
    <span class="n">test_docs</span> <span class="o">=</span> <span class="p">[(</span><span class="n">tokenize</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">row</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span> <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">test_data</span><span class="p">]</span>
    <span class="c1"># JSON 파일로 저장
</span>    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">'train_docs.json'</span><span class="p">,</span> <span class="s">'w'</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s">"utf-8"</span><span class="p">)</span> <span class="k">as</span> <span class="n">make_file</span><span class="p">:</span>
        <span class="n">json</span><span class="p">.</span><span class="n">dump</span><span class="p">(</span><span class="n">train_docs</span><span class="p">,</span> <span class="n">make_file</span><span class="p">,</span> <span class="n">ensure_ascii</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="s">"</span><span class="se">\t</span><span class="s">"</span><span class="p">)</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">'test_docs.json'</span><span class="p">,</span> <span class="s">'w'</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s">"utf-8"</span><span class="p">)</span> <span class="k">as</span> <span class="n">make_file</span><span class="p">:</span>
        <span class="n">json</span><span class="p">.</span><span class="n">dump</span><span class="p">(</span><span class="n">test_docs</span><span class="p">,</span> <span class="n">make_file</span><span class="p">,</span> <span class="n">ensure_ascii</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="s">"</span><span class="se">\t</span><span class="s">"</span><span class="p">)</span>

<span class="c1"># 예쁘게(?) 출력하기 위해서 pprint 라이브러리 사용
</span><span class="n">pprint</span><span class="p">(</span><span class="n">train_docs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="s">'''
OUTPUT:
[['아/Exclamation',
  '더빙/Noun',
  '../Punctuation',
  '진짜/Noun',
  '짜증나다/Adjective',
  '목소리/Noun'],
 '0']
'''</span>
</code></pre></div></div>

<p>분석한 데이터의 토큰(문자열을 분석을 위한 작은 단위)의 갯수를 확인해봅시다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">t</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">train_docs</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">d</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
<span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">))</span>

<span class="s">'''
OUTPUT:
2159921
'''</span>
</code></pre></div></div>

<p>이제 이 데이터를 nltk 라이브러리를 통해서 전처리를 해볼텐데요, <code class="language-plaintext highlighter-rouge">Text</code> 클래스는 문서를 편리하게 탐색할 수 있는 다양한 기능을 제공합니다.</p>

<p>여기에서는 <code class="language-plaintext highlighter-rouge">vocab().most_common</code> 메서드를 이용해서 데이터에서 가장 자주 사용되는 단어를 가져올 때 사용하겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">nltk</span>
<span class="n">text</span> <span class="o">=</span> <span class="n">nltk</span><span class="p">.</span><span class="n">Text</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">'NMSC'</span><span class="p">)</span>

<span class="c1"># 전체 토큰의 개수
</span><span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">.</span><span class="n">tokens</span><span class="p">))</span>

<span class="c1"># 중복을 제외한 토큰의 개수
</span><span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">text</span><span class="p">.</span><span class="n">tokens</span><span class="p">)))</span>            

<span class="c1"># 출현 빈도가 높은 상위 토큰 10개
</span><span class="n">pprint</span><span class="p">(</span><span class="n">text</span><span class="p">.</span><span class="n">vocab</span><span class="p">().</span><span class="n">most_common</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>

<span class="s">'''
OUTPUT:
2159921
49895
[('./Punctuation', 67778),
 ('영화/Noun', 50818),
 ('하다/Verb', 41209),
 ('이/Josa', 38540),
 ('보다/Verb', 38538),
 ('의/Josa', 30188),
 ('../Punctuation', 29055),
 ('가/Josa', 26627),
 ('에/Josa', 26468),
 ('을/Josa', 23118)]
'''</span>
</code></pre></div></div>

<p>자주 나오는 단어 50개를 <code class="language-plaintext highlighter-rouge">matplotlib</code> 라이브러리를 통해서 그래프로 나타내보겠습니다.</p>

<p>한편 한글 폰트를 로드해야 글씨가 깨지지 않고 출력이 되는데요,</p>

<p>윈도우에서는 <code class="language-plaintext highlighter-rouge">font_fname</code> 을 <code class="language-plaintext highlighter-rouge">'c:/windows/fonts/gulim.ttc'</code>,</p>

<p>리눅스에서는 <code class="language-plaintext highlighter-rouge">/usr/share/fonts/nanumfont/NanumGothic.ttf</code> 등 한글 폰트를 지정해줘야 합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">font_manager</span><span class="p">,</span> <span class="n">rc</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="n">font_fname</span> <span class="o">=</span> <span class="s">'/Library/Fonts/AppleGothic.ttf'</span>
<span class="n">font_name</span> <span class="o">=</span> <span class="n">font_manager</span><span class="p">.</span><span class="n">FontProperties</span><span class="p">(</span><span class="n">fname</span><span class="o">=</span><span class="n">font_fname</span><span class="p">).</span><span class="n">get_name</span><span class="p">()</span>
<span class="n">rc</span><span class="p">(</span><span class="s">'font'</span><span class="p">,</span> <span class="n">family</span><span class="o">=</span><span class="n">font_name</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">text</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/img/posts/ml-with-python/nsmc_plot.png" alt="" class="center-95" /><br />
<span class="caption"></span></p>

<p>이제 자주 사용되는 토큰 10000개를 사용해서 데이터를 벡터화를 시키겠습니다.</p>

<p>여기서는 <strong>원 핫 인코딩</strong> 대신에 <strong>CountVectorization</strong>을 사용했습니다.</p>

<p>이는 문서 집합에서 단어 토큰을 생성하고 각 단어의 수를 세어 BOW(Bag of Words) 인코딩한 벡터를 만드는 역할을 합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 시간이 꽤 걸립니다! 시간을 절약하고 싶으면 most_common의 매개변수를 줄여보세요.
</span><span class="n">selected_words</span> <span class="o">=</span> <span class="p">[</span><span class="n">f</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">text</span><span class="p">.</span><span class="n">vocab</span><span class="p">().</span><span class="n">most_common</span><span class="p">(</span><span class="mi">10000</span><span class="p">)]</span>

<span class="k">def</span> <span class="nf">term_frequency</span><span class="p">(</span><span class="n">doc</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">doc</span><span class="p">.</span><span class="n">count</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">selected_words</span><span class="p">]</span>

<span class="n">train_x</span> <span class="o">=</span> <span class="p">[</span><span class="n">term_frequency</span><span class="p">(</span><span class="n">d</span><span class="p">)</span> <span class="k">for</span> <span class="n">d</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">train_docs</span><span class="p">]</span>
<span class="n">test_x</span> <span class="o">=</span> <span class="p">[</span><span class="n">term_frequency</span><span class="p">(</span><span class="n">d</span><span class="p">)</span> <span class="k">for</span> <span class="n">d</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">test_docs</span><span class="p">]</span>
<span class="n">train_y</span> <span class="o">=</span> <span class="p">[</span><span class="n">c</span> <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">train_docs</span><span class="p">]</span>
<span class="n">test_y</span> <span class="o">=</span> <span class="p">[</span><span class="n">c</span> <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">test_docs</span><span class="p">]</span>
</code></pre></div></div>

<p>이제 데이터를 float로 형 변환 시켜주면 데이터 전처리 과정은 끝납니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">x_train</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">train_x</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="s">'float32'</span><span class="p">)</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">test_x</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="s">'float32'</span><span class="p">)</span>

<span class="n">y_train</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">train_y</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="s">'float32'</span><span class="p">)</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">test_y</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="s">'float32'</span><span class="p">)</span>
</code></pre></div></div>

<p><br /></p>

<h2 id="모델-정의-및-학습하기">모델 정의 및 학습하기</h2>

<hr />

<p>IMDB 영화 리뷰 분석에서 사용했던 모델을 조금 변형했습니다.</p>

<p>모델의 구조는 다음의 그림과 같습니다.</p>

<p><img src="/img/posts/ml-with-python/nsmc_structure.png" alt="" class="center-50" /><br />
<span class="caption"></span></p>

<p>두 개의 Dense 층은 64개의 유닛을 가지고 활성화 함수로는 relu를 사용했으며, 마지막 층은 sigmoid 활성화 함수를 사용해서 긍정의 리뷰일 확률을 출력합니다.</p>

<p>손실 함수로는 <code class="language-plaintext highlighter-rouge">binary_crossentropy</code>를 사용했고 <code class="language-plaintext highlighter-rouge">RMSProp</code> 옵티마이저를 통해서 경사하강법을 진행했습니다.</p>

<p>또한 배치 사이즈를 512로, 에포크를 10번으로 학습시켰습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">models</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">layers</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">optimizers</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">losses</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">metrics</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="p">.</span><span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">10000</span><span class="p">,)))</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'sigmoid'</span><span class="p">))</span>

<span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">optimizers</span><span class="p">.</span><span class="n">RMSprop</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">),</span>
             <span class="n">loss</span><span class="o">=</span><span class="n">losses</span><span class="p">.</span><span class="n">binary_crossentropy</span><span class="p">,</span>
             <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">metrics</span><span class="p">.</span><span class="n">binary_accuracy</span><span class="p">])</span>

<span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>

<span class="s">'''
OUTPUT:
Epoch 1/10
150000/150000 [==============================] - 17s 112us/step - loss: 0.3878 - binary_accuracy: 0.8344
Epoch 2/10
150000/150000 [==============================] - 14s 96us/step - loss: 0.3170 - binary_accuracy: 0.8641
Epoch 3/10
150000/150000 [==============================] - 14s 91us/step - loss: 0.2931 - binary_accuracy: 0.8774
Epoch 4/10
150000/150000 [==============================] - 15s 97us/step - loss: 0.2730 - binary_accuracy: 0.8886
Epoch 5/10
150000/150000 [==============================] - 15s 100us/step - loss: 0.2538 - binary_accuracy: 0.8983
Epoch 6/10
150000/150000 [==============================] - 16s 108us/step - loss: 0.2340 - binary_accuracy: 0.9074
Epoch 7/10
150000/150000 [==============================] - 18s 122us/step - loss: 0.2147 - binary_accuracy: 0.9164
Epoch 8/10
150000/150000 [==============================] - 16s 107us/step - loss: 0.1956 - binary_accuracy: 0.9253
Epoch 9/10
150000/150000 [==============================] - 19s 125us/step - loss: 0.1775 - binary_accuracy: 0.9323
Epoch 10/10
150000/150000 [==============================] - 18s 120us/step - loss: 0.1624 - binary_accuracy: 0.9386
50000/50000 [==============================] - 6s 123us/step
'''</span>
</code></pre></div></div>

<p>테스트 데이터로 확인해본 결과를 출력해보면 85%의 성능을 보여주는 것을 알 수 있습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">results</span>

<span class="s">'''OUTPUT:
[0.41844128477334974, 0.8525]
'''</span>
</code></pre></div></div>

<p><br /></p>

<h2 id="새로운-데이터로-결과-예측하기">새로운 데이터로 결과 예측하기</h2>

<hr />

<p>이제 문자열 형태의 새로운 데이터를 받아와서 바로 결과를 예측하는 함수를 만들어 보겠습니다.</p>

<p>데이터의 형태를 맞춰주기 위해서 <code class="language-plaintext highlighter-rouge">np.expand_dims</code> 메서드를 이용해 array의 축을 확장시켰습니다.</p>

<p>최종 확률이 0.5 보다 크면 긍정이고, 그렇지 않으면 부정이라고 예측했습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">predict_pos_neg</span><span class="p">(</span><span class="n">review</span><span class="p">):</span>
    <span class="n">token</span> <span class="o">=</span> <span class="n">tokenize</span><span class="p">(</span><span class="n">review</span><span class="p">)</span>
    <span class="n">tf</span> <span class="o">=</span> <span class="n">term_frequency</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">tf</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="s">'float32'</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">score</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>
    <span class="k">if</span><span class="p">(</span><span class="n">score</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">):</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"[{}]는 {:.2f}% 확률로 긍정 리뷰이지 않을까 추측해봅니다.^^</span><span class="se">\n</span><span class="s">"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">review</span><span class="p">,</span> <span class="n">score</span> <span class="o">*</span> <span class="mi">100</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"[{}]는 {:.2f}% 확률로 부정 리뷰이지 않을까 추측해봅니다.^^;</span><span class="se">\n</span><span class="s">"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">review</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">score</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span><span class="p">))</span>
</code></pre></div></div>

<p>이렇게 여러 가지 한글 리뷰를 매개변수로 넣어서 예측할 수 있습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">predict_pos_neg</span><span class="p">(</span><span class="s">"올해 최고의 영화! 세 번 넘게 봐도 질리지가 않네요."</span><span class="p">)</span>
<span class="n">predict_pos_neg</span><span class="p">(</span><span class="s">"배경 음악이 영화의 분위기랑 너무 안 맞았습니다. 몰입에 방해가 됩니다."</span><span class="p">)</span>
<span class="n">predict_pos_neg</span><span class="p">(</span><span class="s">"주연 배우가 신인인데 연기를 진짜 잘 하네요. 몰입감 ㅎㄷㄷ"</span><span class="p">)</span>
<span class="n">predict_pos_neg</span><span class="p">(</span><span class="s">"믿고 보는 감독이지만 이번에는 아니네요"</span><span class="p">)</span>
<span class="n">predict_pos_neg</span><span class="p">(</span><span class="s">"주연배우 때문에 봤어요"</span><span class="p">)</span>

<span class="s">'''
OUTPUT:
[올해 최고의 영화! 세 번 넘게 봐도 질리지가 않네요.]는 98.49% 확률로 긍정 리뷰이지 않을까 추측해봅니다.^^

[배경 음악이 영화의 분위기랑 너무 안 맞았습니다. 몰입에 방해가 됩니다.]는 91.99% 확률로 부정 리뷰이지 않을까 추측해봅니다.^^;

[주연 배우가 신인인데 연기를 진짜 잘 하네요. 몰입감 ㅎㄷㄷ]는 99.25% 확률로 긍정 리뷰이지 않을까 추측해봅니다.^^

[믿고 보는 감독이지만 이번에는 아니네요]는 61.15% 확률로 부정 리뷰이지 않을까 추측해봅니다.^^;

[주연배우 때문에 봤어요]는 74.03% 확률로 부정 리뷰이지 않을까 추측해봅니다.^^;
'''</span>
</code></pre></div></div>

<p><br /></p>

<h3 id="참조">참조</h3>

<hr />

<ul>
  <li><a href="https://www.lucypark.kr/docs/2015-pyconkr/#1">한국어와 NLTK, Gensim의 만남</a></li>
  <li><a href="http://konlpy.org/ko/v0.5.1/">KoNLPy 공식 문서</a></li>
  <li><a href="http://www.yes24.com/24/Goods/65050162?Acode=101">케라스 창시자에게 배우는 딥러닝 (Machine Learning with Python)</a></li>
</ul>

<p><br /></p>
:ET