<!DOCTYPE html>
<html lang="en">
<head>
  <!-- <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script> -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  
  <!-- Favicon code from realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#8b51a3">
<meta name="msapplication-TileColor" content="#563d7c">
<meta name="theme-color" content="#ffffff">

  <!--jQuery-->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

  <!-- Fonts & Icons -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
  <link href='//spoqa.github.io/spoqa-han-sans/css/SpoqaHanSans-kr.css' rel='stylesheet' type='text/css'>

  <!-- CSS -->
  <link rel="stylesheet" href="/assets/css/main.css">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>From Information to Divergence | Daeyoung Kim</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="From Information to Divergence" />
<meta name="author" content="cyc1am3n" />
<meta property="og:locale" content="en_GB" />
<meta name="description" content="Self-information(정보량)" />
<meta property="og:description" content="Self-information(정보량)" />
<meta property="og:site_name" content="Daeyoung Kim" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-03-01T22:30:00+09:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="From Information to Divergence" />
<meta name="twitter:site" content="@daeyoung__k" />
<meta name="twitter:creator" content="@cyc1am3n" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"cyc1am3n"},"dateModified":"2020-03-01T22:30:00+09:00","datePublished":"2020-03-01T22:30:00+09:00","description":"Self-information(정보량)","headline":"From Information to Divergence","mainEntityOfPage":{"@type":"WebPage","@id":"/2020/03/01/from_information_to_divergence.html"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"/assets/img/smile.png"},"name":"cyc1am3n"},"url":"/2020/03/01/from_information_to_divergence.html"}</script>
<!-- End Jekyll SEO tag -->

</head>
<!--jQuery-->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
<body>
  <div class="container">
    

<section id="header-nav">
  <header>
    <nav>
      <ul>
        
        <!-- others -->
        <a href="/">
          <li class="btn-nav">Home</li>
        </a>
        
        
        
          <a href="/blog">
            <li class="current btn-nav">Blog</li>
          </a>
          <a href="/tags">
            <li class="btn-nav">Tags</li>
          </a>
        
        
      </ul>
    </nav>
  </header>
</section>
<div id="post">
  <section class="post-header">
    <h1 class="title">From Information to Divergence</h1>
    <p class="subtitle">Self-information, Entropy, Cross Entropy, KL-divergence, F-divergence</p>
    <p class="meta">
      March 1, 2020
    </p>
  </section>
  <section class="post-content">
    <h3 id="self-information정보량">Self-information(정보량)</h3>

<ul>
  <li>Information theory 에서 information은 특정한 관찰에 의해 얼마만큼의 정보를 획득했는지 수치로 정량화한 값이다.</li>
  <li>모델 학습에 있어서 얼마나 영향력 있는지, 정보의 파급력 또는 놀람의 정도로 해석할 수 있다.
    <ul>
      <li>즉, 큰 정보량은 자주 발생하는 관찰이나 사건에 대해서는 작은 값을 갖고 자주 발생하지 않는 사건에 대해서는 큰 값을 갖는다.</li>
    </ul>
  </li>
  <li>정보이론에서는 자주 일어나지 않는 사건의 정보량은 자주 발생하는 사건보다 정보량이 많다고 간주한다.</li>
  <li>관찰이나 사건 \(A\)의 정보량 \(h(A)\) 를 다음과 같이 정의한다.</li>
</ul>

\[h(A):=-\log P(A)\]

<ul>
  <li>여기에서 \(P(A)\)는 사건 \(A\)의 확률을 의미한다.</li>
</ul>

<p><br /></p>

<h3 id="entropy">Entropy</h3>

<ul>
  <li>Entropy는 확률변수의 평균 정보량, 즉 평균적인 놀람(불확실성)의 정도를 나타낸다.</li>
  <li>이산확률 변수 \(X\)의 평균 정보량 \(H[X]\)는 다음과 같이 정의한다.</li>
</ul>

\[H[X]:=-\sum_{i=1}^N p_i \log p_i\]

<ul>
  <li>연속확률 변수 \(X\)의 평균 정보량 \(H[X]\)는 다음과 같이 정의한다.</li>
</ul>

\[H[X]:=-\int p(x)\log (p(x))dx\]

<p><br /></p>

<h3 id="cross-entropy">Cross Entropy</h3>

<ul>
  <li>두 확률 분포 \(p\) 와 \(q\) 에 대해서 분포 \(p\) 대신에 \(q\)를 사용해 분포 \(p\)를 설명할 때 필요한 정보량을 Cross Entropy라고 한다.</li>
  <li>주어진 확률변수 \(X\)에 대해서 확률분포 \(p\)를 찾는 문제를 생각해보자.
    <ul>
      <li>확률분포 \(p\)의 정확한 형태를 모르기 때문에 \(p\)를 예측한 근사 분포 \(q\)를 이용해야 한다.</li>
    </ul>
  </li>
  <li>Cross-entropy는 다음과 같다.</li>
</ul>

\[\text{Cross-entropy}=-\sum_{i=1}^N p_i \log q_i\]

<ul>
  <li>정보를 나타내는 \(\log\) 값에 \(p(x)\) 대신 \(q(x)\) 를 사용한 것을 볼 수 있다.</li>
</ul>

<p><br /></p>

<h3 id="kullback-leibler-divergencekl-divergence">Kullback-Leibler divergence(KL-divergence)</h3>

<ul>
  <li>두 확률분포의 유사한 정도를 계산하는 방법 중의 하나이다.</li>
  <li>KL Divergence의 정의는 다음과 같다.</li>
</ul>

\[KL(p|q):=-\sum_{i=1}^N p_i \log q_i - \left( -\sum_{i=1}^N p_i \log p_q \right)=-\sum_{i=1}^N p_i \log \left(q_i \over p_i\right)\]

<ul>
  <li>Cross Entropy에 Entropy를 뺀 값이라고 생각하면 된다.</li>
  <li>KL Divergence의 성질은 다음과 같다.
    <ul>
      <li>\(KL(p\vert q)≠KL(q\vert p)\) (non-symmetric).</li>
      <li>\(KL(p\vert q)=0\) if and only if \(p=q\).</li>
      <li>\(KL(p\vert q)≥0\).</li>
    </ul>
  </li>
  <li>KL Divergence를 최소화하는 것은 결국 Cross Entropy를 최소화하는 것과 같으므로 CE를 loss로 사용한다.</li>
</ul>

<p><br /></p>

<h3 id="f-divergence">F-divergence</h3>

<ul>
  <li>두 확률분포의 유사도를 일반화한 형태의 함수이다.</li>
  <li>정의는 다음과 같다.</li>
</ul>

\[D_f(P||Q)=\int q(x)f\left({p(x) \over q(x)}\right)dx\]

<ul>
  <li>여기에서 \(f\)는 convex  function이며 \(f(1)=0\) 을 만족해야한다.</li>
  <li>\(f\) 에 따라서 다양한 divergence를 만들 수 있다.</li>
</ul>

<table>
  <thead>
    <tr>
      <th>Divergence</th>
      <th>Corresponding <em>f(t)</em></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>KL-divergence</td>
      <td>\(t\log t\)</td>
    </tr>
    <tr>
      <td>reverse KL-divergence</td>
      <td>\(-\log t\)</td>
    </tr>
    <tr>
      <td>squared Hellinger distance</td>
      <td>\(({\sqrt {t}}-1)^{2},\,2(1-{\sqrt {t}})\)</td>
    </tr>
    <tr>
      <td>Total variation distance</td>
      <td>\({\frac {1}{2}}\vert t - 1\vert\)</td>
    </tr>
    <tr>
      <td>Pearson \(\chi ^{2}\)-divergence</td>
      <td>\((t-1)^{2},\,t^{2}-1,\,t^{2}-t\)</td>
    </tr>
    <tr>
      <td>Neyman \(\chi ^{2}\)-divergence (reverse Pearson)</td>
      <td>\({\frac {1}{t}}-1,\,{\frac {1}{t}}-t\)</td>
    </tr>
    <tr>
      <td>α-divergence</td>
      <td>\({\begin{cases}{\frac {4}{1-\alpha ^{2}}}{\big (}1-t^{(1+\alpha )/2}{\big )},&amp;{\text{if}}\ \alpha \neq \pm 1,\\t\ln t,&amp;{\text{if}}\ \alpha =1,\\-\ln t,&amp;{\text{if}}\ \alpha =-1\end{cases}}\)</td>
    </tr>
    <tr>
      <td>α-divergence (other designation)</td>
      <td>\(\begin{cases}{\frac {t^{\alpha }-t}{\alpha (\alpha -1)}},&amp;{\text{if}}\ \alpha \neq 0,\,\alpha \neq 1,\\t\ln t,&amp;{\text{if}}\ \alpha =1,\\-\ln t,&amp;{\text{if}}\ \alpha =0\end{cases}\)</td>
    </tr>
  </tbody>
</table>

<hr />

<blockquote>
  <p><strong>Reference:</strong></p>

  <ul>
    <li><a href="https://icim.nims.re.kr/post/easyMath/550">알기 쉬운 산업 수학 - Entropy, Cross-entropy, KL Divergence</a></li>
    <li><a href="https://reniew.github.io/17/">reniew’s blog - 정보이론 : 엔트로피, KL-Divergence</a></li>
    <li><a href="https://en.wikipedia.org/wiki/F-divergence">Wikipedia - f divergence</a></li>
  </ul>
</blockquote>


  </section>
</div>

<div id="top" class="top-btn" onclick="moveTop()">
  <i class="fas fa-chevron-up"></i>
</div>

<script>
  var lastScrollTop = 0;
  window.onscroll = function () {
    var st = document.body.scrollTop || document.documentElement.scrollTop;
    if (st > 250) {
      document.getElementById("top").style.display = "block"
      if (st > lastScrollTop) {
        document.getElementById("top").style.opacity = 0
      } else {
        document.getElementById("top").style.opacity = 1
      }
    } else {
      document.getElementById("top").style.opacity = 0
      if (st > lastScrollTop) {
        document.getElementById("top").style.display = "none"
      }
    }
    lastScrollTop = st <= 0 ? 0 : st;
  }
  function moveTop() {
    document.body.scrollTop = 0
    document.documentElement.scrollTop = 0
  }
</script>

<!-- Footer -->
<footer>
  <div class="footer">
    Copyright © 2022
    <a href="https://cyc1am3n.github.io">Daeyoung Kim</a>.
  </div>
</footer>

  </div>
</body>

</html>