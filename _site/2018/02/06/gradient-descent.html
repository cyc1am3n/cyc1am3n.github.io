<!DOCTYPE html>
<html lang="en">
<head>
  <!-- <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script> -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  
  <!-- Favicon code from realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#8b51a3">
<meta name="msapplication-TileColor" content="#563d7c">
<meta name="theme-color" content="#ffffff">

  <!--jQuery-->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

  <!-- Fonts & Icons -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
  <link href='//spoqa.github.io/spoqa-han-sans/css/SpoqaHanSans-kr.css' rel='stylesheet' type='text/css'>

  <!-- CSS -->
  <link rel="stylesheet" href="/assets/css/main.css">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>[Machine Learning] Gradient Descent | Daeyoung Kim</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="[Machine Learning] Gradient Descent" />
<meta name="author" content="cyc1am3n" />
<meta property="og:locale" content="en_GB" />
<meta name="description" content="* 이 포스트는 Coursera에 있는 Andrew Ng 교수님의 강의 Machine Learning(링크)를 바탕으로 작성되었습니다." />
<meta property="og:description" content="* 이 포스트는 Coursera에 있는 Andrew Ng 교수님의 강의 Machine Learning(링크)를 바탕으로 작성되었습니다." />
<link rel="canonical" href="http://localhost:4000/2018/02/06/gradient-descent.html" />
<meta property="og:url" content="http://localhost:4000/2018/02/06/gradient-descent.html" />
<meta property="og:site_name" content="Daeyoung Kim" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-02-06T17:17:54+09:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="[Machine Learning] Gradient Descent" />
<meta name="twitter:site" content="@daeyoung__k" />
<meta name="twitter:creator" content="@cyc1am3n" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"cyc1am3n"},"dateModified":"2018-02-06T17:17:54+09:00","datePublished":"2018-02-06T17:17:54+09:00","description":"* 이 포스트는 Coursera에 있는 Andrew Ng 교수님의 강의 Machine Learning(링크)를 바탕으로 작성되었습니다.","headline":"[Machine Learning] Gradient Descent","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2018/02/06/gradient-descent.html"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/assets/img/smile.png"},"name":"cyc1am3n"},"url":"http://localhost:4000/2018/02/06/gradient-descent.html"}</script>
<!-- End Jekyll SEO tag -->

</head>
<!--jQuery-->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
<body>
  <div class="container">
    

<section id="header-nav">
  <header>
    <nav>
      <ul>
        
        <!-- others -->
        <a href="/">
          <li class="btn-nav">Home</li>
        </a>
        
        
        
          <a href="/blog">
            <li class="current btn-nav">Blog</li>
          </a>
          <a href="/tags">
            <li class="btn-nav">Tags</li>
          </a>
        
        
      </ul>
    </nav>
  </header>
</section>
<div id="post">
  <section class="post-header">
    <h1 class="title">[Machine Learning] Gradient Descent</h1>
    <p class="subtitle">「 Machine Learning 」 lecture summary #3</p>
    <p class="meta">
      February 6, 2018
    </p>
  </section>
  <section class="post-content">
    <p>* 이 포스트는 Coursera에 있는 Andrew Ng 교수님의 강의 <a href="https://www.coursera.org/learn/machine-learning">Machine Learning(링크)</a>를 바탕으로 작성되었습니다.</p>

<h2 id="gradient-descent">Gradient Descent</h2>

<p>저번 포스팅에서 언급했던 Cost Function의 최소값을 다시 한 번 생각해보자.</p>

<p>다음은 θ<sub>0</sub>을 x축, θ<sub>1</sub>을 y축, J(θ<sub>0</sub>, θ<sub>1</sub>)을 z축으로 하는 그래프의 예이다.</p>

<p style="text-align: center;"><img src="/assets/img/posts/gradient-descent/gradient-descent-1.png" alt="그림1" width="75%" height="75%" /></p>

<p>이 그래프에서 가장 아래있는 점의 x, y 좌표 즉 θ<sub>0</sub>와, θ<sub>1</sub>을 알아내는 것이 좋은 hypothesis function 만들기의 목표이다.</p>

<p>이 최소값은 cost function의 미분을 통해서 구할 수 있는데, 어떤 점에서 미분을 해서 나온 기울기 값이 최소값으로 향하는 방향을 제시한다는 것이다.</p>

<p>이런 방법으로 θ를 구하는 게 <code class="language-plaintext highlighter-rouge">gradient descent</code> 의 algorithm이다.</p>

<p>한편 이렇게 미분을 통해 나온 기울기 값으로 점점 최소값으로 향할 때 얼마만큼 이동할지는 <code class="language-plaintext highlighter-rouge">learning rate</code>인 상수값으로 설정하는데 보통 learning rate는 α라고 쓴다.</p>

<p>learning rate 이야기는 잠시 후에 하고 일단 이렇게 cost function의 최소값을 구하는 gradient descent의 algorithm을 살펴보자.</p>

<p><img src="/assets/img/posts/gradient-descent/gradient-descent-2.png" alt="그림2" width="40%" height="40%" /></p>

<p>위의 알고리즘은 각 θ에 cost function을 편미분한 값에 α를 곱한만큼 반복해서 뺀다.</p>

<p>여기서 중요한 부분은 각 θ에 대해 <code class="language-plaintext highlighter-rouge">simultaneous update</code>를 해줘야한다는 점이다.</p>

<p style="text-align: center;"><img src="/assets/img/posts/gradient-descent/gradient-descent-3.png" alt="그림3" width="80%" height="80%" /></p>

<p>위를 보면 simultaneous update와 그렇지 않은 update의 차이를 알 수 있을 것이다.</p>

<p>이제 learning rate를 어떻게 설정해야 하는지 생각해보자.</p>

<p>learning rate가 작으면 θ 값이 작게 변하므로 cost function의 최소값에 가는데 오래 걸릴 것이고,</p>

<p>learning rate가 클수록 θ 값이 크게 변하므로 cost function의 최소값에 금방 도달할 수 있을 것이다.</p>

<p>하지만 learning rate가 어느 정도 이상으로 커지면 다음과 같은 문제가 발생한다.</p>

<p style="text-align: center;"><img src="/assets/img/posts/gradient-descent/gradient-descent-4.png" alt="그림4" width="75%" height="75%" /></p>

<p>learning rate가 너무 커지면 위와 같이 최소값에 도달 하지 않을 수도 있다는 것이다.</p>

<p>따라서 적당한 learning rate를 설정해야하는데, 이건 data set마다 변화하는 정도가 다르므로 거기에 맞게 정해줘야한다.</p>

<p>또한 위에서 learning rate는 상수라고 했었는데 최소값에 더 정밀하게 다가가기 위해서 변수로 지정하지 않아도 괜찮다.</p>

<p>왜냐하면 최소값에 가까워질수록 각각 점에서의 기울기가 감소하므로 천천히 최소값에 접근하기 때문이다.</p>

<p>learning rate에 대해서는 다음 포스팅에서도 이야기를 할 것이다.</p>

<p style="text-align: center;"><img src="/assets/img/posts/gradient-descent/gradient-descent-5.png" alt="그림5" width="85%" height="85%" /></p>

<p>사실 위에서 보여준 예시는 다 Linear Regression이긴 한데, 눈으로 보여주기가 용이해서 예시를 든 것이지 다른 hypothesis function에도 똑같이 적용이 된다.</p>

<h2 id="gradient-descent-for-linear-regression">Gradient Descent For Linear Regression</h2>

<p>이제 Linear Regression의 Gradient Descent에 대해서 집중적으로 생각해보자.</p>

<p>traning set이 1개라고 했을 때, Cost function을 편미분한 값은 다음과 같다.</p>

<p style="text-align: center;"><img src="/assets/img/posts/gradient-descent/gradient-descent-6.png" alt="그림6" width="55%" height="55%" /></p>

<p>여기서 x<sub>0</sub>, x<sub>1</sub>은 편의상 h(x) = θ<sub>0</sub> + θ<sub>1</sub>x = θ<sub>0</sub>x<sub>0</sub> + θ<sub>1</sub>x<sub>1</sub>으로 표기했으며, x<sub>0</sub> = 1이다.</p>

<p>이를 Gradient Descent algorithm에 적용하면,</p>

<p style="text-align: center;"><img src="/assets/img/posts/gradient-descent/gradient-descent-7.png" alt="그림7" width="65%" height="65%" /></p>

<p>이렇게 나오는데 이 algorithm대로 θ를 구하면 된다.</p>

<p>그런데 Gradient Descent는 <code class="language-plaintext highlighter-rouge">local optimum</code>을 구하는 algorithm이다.</p>

<p>하지만 local optimum은 가장 좋은 θ가 아니라 global optimum일때 cost function이 최소가 되므로 그 때의 θ가 가장 적합하다.</p>

<p>그래서 Gradient Descent는 <code class="language-plaintext highlighter-rouge">convex</code>한 cost function에서만 해를 구할 수 있다.</p>

<p>Linear Regression에서 cost function은 quadratic 하므로 local optimum이 global optimum이고 Gradient Descent를 이용하면 최적의 hypothesis function을 만들 수 있다.</p>

<p>다음 포스팅에서는 <code class="language-plaintext highlighter-rouge">Multivariate Linear Regression</code>에 대해서 알아보겠다.</p>

  </section>
</div>

<div id="top" class="top-btn" onclick="moveTop()">
  <i class="fas fa-chevron-up"></i>
</div>

<script>
  var lastScrollTop = 0;
  window.onscroll = function () {
    var st = document.body.scrollTop || document.documentElement.scrollTop;
    if (st > 250) {
      document.getElementById("top").style.display = "block"
      if (st > lastScrollTop) {
        document.getElementById("top").style.opacity = 0
      } else {
        document.getElementById("top").style.opacity = 1
      }
    } else {
      document.getElementById("top").style.opacity = 0
      if (st > lastScrollTop) {
        document.getElementById("top").style.display = "none"
      }
    }
    lastScrollTop = st <= 0 ? 0 : st;
  }
  function moveTop() {
    document.body.scrollTop = 0
    document.documentElement.scrollTop = 0
  }
</script>

<!-- Footer -->
<footer>
  <div class="footer">
    Copyright © 2022
    <a href="https://cyc1am3n.github.io">Daeyoung Kim</a>.
  </div>
</footer>

  </div>
</body>

</html>