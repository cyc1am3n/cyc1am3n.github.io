<!DOCTYPE html>
<html lang="en">
<head>
  <!-- <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script> -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  
  <!-- Favicon code from realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#8b51a3">
<meta name="msapplication-TileColor" content="#563d7c">
<meta name="theme-color" content="#ffffff">

  <!--jQuery-->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

  <!-- Fonts & Icons -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
  <link href='//spoqa.github.io/spoqa-han-sans/css/SpoqaHanSans-kr.css' rel='stylesheet' type='text/css'>

  <!-- CSS -->
  <link rel="stylesheet" href="/assets/css/main.css">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>[Coursera] Art and Science of Machine Learning (1) | Daeyoung Kim</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="[Coursera] Art and Science of Machine Learning (1)" />
<meta name="author" content="cyc1am3n" />
<meta property="og:locale" content="en_GB" />
<meta name="description" content="Coursera 강의 “Machine Learning with TensorFlow on Google Cloud Platform” 중 다섯 번째 코스인 Art and Science of Machine Learning의 강의노트입니다." />
<meta property="og:description" content="Coursera 강의 “Machine Learning with TensorFlow on Google Cloud Platform” 중 다섯 번째 코스인 Art and Science of Machine Learning의 강의노트입니다." />
<link rel="canonical" href="http://localhost:4000/2018/10/25/art-and-science-of-ml.html" />
<meta property="og:url" content="http://localhost:4000/2018/10/25/art-and-science-of-ml.html" />
<meta property="og:site_name" content="Daeyoung Kim" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-10-25T23:00:54+09:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="[Coursera] Art and Science of Machine Learning (1)" />
<meta name="twitter:site" content="@daeyoung__k" />
<meta name="twitter:creator" content="@cyc1am3n" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"cyc1am3n"},"dateModified":"2018-10-25T23:00:54+09:00","datePublished":"2018-10-25T23:00:54+09:00","description":"Coursera 강의 “Machine Learning with TensorFlow on Google Cloud Platform” 중 다섯 번째 코스인 Art and Science of Machine Learning의 강의노트입니다.","headline":"[Coursera] Art and Science of Machine Learning (1)","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2018/10/25/art-and-science-of-ml.html"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/assets/img/smile.png"},"name":"cyc1am3n"},"url":"http://localhost:4000/2018/10/25/art-and-science-of-ml.html"}</script>
<!-- End Jekyll SEO tag -->

</head>
<!--jQuery-->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
<body>
  <div class="container">
    

<section id="header-nav">
  <header>
    <nav>
      <ul>
        
        <!-- others -->
        <a href="/">
          <li class="btn-nav">Home</li>
        </a>
        
        
        
          <a href="/blog">
            <li class="current btn-nav">Blog</li>
          </a>
          <a href="/tags">
            <li class="btn-nav">Tags</li>
          </a>
        
        
      </ul>
    </nav>
  </header>
</section>
<div id="post">
  <section class="post-header">
    <h1 class="title">[Coursera] Art and Science of Machine Learning (1)</h1>
    <p class="subtitle">Machine Learning with TensorFlow on GCP</p>
    <p class="meta">
      October 25, 2018
    </p>
  </section>
  <section class="post-content">
    <p>Coursera 강의 “Machine Learning with TensorFlow on Google Cloud Platform” 중 다섯 번째 코스인 <a href="https://www.coursera.org/learn/art-science-ml/home/welcome">Art and Science of Machine Learning</a>의 강의노트입니다.</p>

<hr />

<p><br /></p>

<h2 id="regularization">Regularization</h2>

<hr />

<p><img src="/img/posts/art-and-science-of-ml/01.png" alt="" class="center-75" /><br />
<span class="caption"></span></p>

<ul>
  <li>
    <p>The <code class="language-plaintext highlighter-rouge">simpler</code> the better</p>
  </li>
  <li>
    <p>Factor in model complexity when calculating error</p>

    <ul>
      <li>Minimize: loss(Data|Model) + complexity(Model)</li>
      <li>loss is aimed for low training error</li>
      <li>but balance against complexity</li>
      <li>
        <p>Optimal model complexity is data-dependent, so requires hyperparameter tuning <code class="language-plaintext highlighter-rouge">Regularization</code> is a major field of ML research</p>
      </li>
      <li>Early Stopping</li>
      <li>Parameter Norm Penalties
        <ul>
          <li><code class="language-plaintext highlighter-rouge">L1 / L2 regularization</code></li>
          <li>Max-norm regularization</li>
        </ul>
      </li>
      <li>Dataset Augmentation</li>
      <li>Noise Robustness</li>
      <li>Sparse Representations</li>
      <li>…</li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h2 id="l1--l2-regularizations">L1 &amp; L2 Regularizations</h2>

<hr />

<ul>
  <li>How can we measure model complexity?</li>
</ul>

<p><img src="/img/posts/art-and-science-of-ml/02.png" alt="L2 vs. L1 Norm" class="center-75" /><br />
<span class="caption">L2 vs. L1 Norm</span></p>

<ul>
  <li>
    <p>In <code class="language-plaintext highlighter-rouge">L2</code> regularization, <code class="language-plaintext highlighter-rouge">complexity</code> of model is defined by the L2 norm of the weight vector</p>

\[L(w,D)+\lambda||w||_{\color{Red}2}\]

    <ul>
      <li><code class="language-plaintext highlighter-rouge">lambda</code> controls how these are balanced</li>
    </ul>
  </li>
  <li>
    <p>In <code class="language-plaintext highlighter-rouge">L1</code> regularization, <code class="language-plaintext highlighter-rouge">complexity</code> of model is defined by the L1 norm of the weight vector</p>

\[L(w,D)+\lambda||w||_{\color{Red}1}\]

    <ul>
      <li>L1 regularization can be used as a <code class="language-plaintext highlighter-rouge">feature selection</code> mechanism</li>
    </ul>
  </li>
</ul>

<h2 id="learning-rate-and-batch-size">Learning rate and batch size</h2>

<hr />

<ul>
  <li>We have several knobs that are <code class="language-plaintext highlighter-rouge">dataset-dependent</code></li>
  <li><code class="language-plaintext highlighter-rouge">Learning rate</code> controls the size of the step in weight space
    <ul>
      <li>If too <code class="language-plaintext highlighter-rouge">small</code>, training will take a <code class="language-plaintext highlighter-rouge">long</code> time</li>
      <li>If too <code class="language-plaintext highlighter-rouge">large</code>, training will <code class="language-plaintext highlighter-rouge">bounce</code> around</li>
      <li>Default learning rate in Estimator’s LinearRegressor is smaller of 0.2 or <code class="language-plaintext highlighter-rouge">1/sqrt(num_features)</code> → this assume that your feature and label values are small numbers</li>
    </ul>
  </li>
  <li>The <code class="language-plaintext highlighter-rouge">batch size</code>  controls the number of samples that gradient is calculated on
    <ul>
      <li>If too <code class="language-plaintext highlighter-rouge">small</code>, training will <code class="language-plaintext highlighter-rouge">bounce</code> around</li>
      <li>If too <code class="language-plaintext highlighter-rouge">large</code>, training will take a very <code class="language-plaintext highlighter-rouge">long</code> time</li>
      <li><code class="language-plaintext highlighter-rouge">40 - 100</code> tends to be a good range for batch size Can go up to as high as 500</li>
    </ul>
  </li>
  <li>Regularization provides a way to define model complexity based on the values of the weights</li>
</ul>

<p><br /></p>

<h2 id="optimization">Optimization</h2>

<hr />

<ul>
  <li><code class="language-plaintext highlighter-rouge">Optimization</code>  is a major field of ML research
    <ul>
      <li><code class="language-plaintext highlighter-rouge">GradientDescent</code> — The traditional approach, typically implemented stochastically i.e. with batches</li>
      <li><code class="language-plaintext highlighter-rouge">Momentum</code> — Reduces learning rate when gradient values are small</li>
      <li><code class="language-plaintext highlighter-rouge">AdaGrad</code> — Give frequently occurring features low learning rates</li>
      <li><code class="language-plaintext highlighter-rouge">AdaDelta</code> — Improves AdaGrad by avoiding reducing LR to zero</li>
      <li><code class="language-plaintext highlighter-rouge">Adam</code> — AdaGrad with a bunch of fixes</li>
      <li><code class="language-plaintext highlighter-rouge">Ttrl</code> — “Follow the regularized leader”, works well on wide models</li>
      <li>…</li>
      <li>Last two things are good defaults for <code class="language-plaintext highlighter-rouge">DNN and Linear</code> models</li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h2 id="practicing-with-tensorflow-code">Practicing with TensorFlow code</h2>

<hr />

<ul>
  <li>How to change optimizer, learning rate, batch size</li>
</ul>

<figure class="highlight"><pre><code class="language-python" data-lang="python"> 
<span class="n">train_fn</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">estimator</span><span class="p">.</span><span class="n">inputs</span><span class="p">.</span><span class="n">pandas_input_fn</span><span class="p">(...,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">myopt</span> <span class="o">=</span> <span class="n">train</span><span class="p">.</span><span class="n">FtrlOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
<span class="err">​</span>							<span class="n">l2_regularization_strength</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">estimator</span><span class="p">.</span><span class="n">LinearRegressor</span><span class="p">(...,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">myopt</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">(</span><span class="n">input_fn</span><span class="o">=</span><span class="n">train_fn</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span></code></pre></figure>

<ol>
  <li>Control <code class="language-plaintext highlighter-rouge">batch size</code> via the input function</li>
  <li>Control <code class="language-plaintext highlighter-rouge">learning rate</code> via the optimizer passed into model</li>
  <li>Set up <code class="language-plaintext highlighter-rouge">regularization</code> in the optimizer</li>
  <li>Adjust number of steps based on batch_size, learning_rate</li>
  <li>Set number of steps. not number of epochs because distributed training doesn’t play nicely with epochs.</li>
</ol>

<p><br /></p>

<h2 id="hyperparameter-tuning">Hyperparameter Tuning</h2>

<hr />

<ul>
  <li>ML models are mathematical functions with parameters and hyper-parameters
    <ul>
      <li><code class="language-plaintext highlighter-rouge">Parameters</code> changed during model training</li>
      <li><code class="language-plaintext highlighter-rouge">Hyper-parameters</code> set before training</li>
    </ul>
  </li>
  <li>Model improvement is very sensitive to batch_size and learning_rate</li>
</ul>

<p><img src="/img/posts/art-and-science-of-ml/03.png" alt="" class="center-75" /><br />
<span class="caption"></span></p>

<ul>
  <li>There are a variety of model parameters too
    <ul>
      <li>Size of model</li>
      <li>Number of hash buckets</li>
      <li>Embedding size</li>
      <li>Etc.</li>
      <li>Wouldn’t it be nice to have the NN training loop do meta-training across all these parameters?</li>
    </ul>
  </li>
  <li>How to use <code class="language-plaintext highlighter-rouge">Cloud ML Engine</code> for hyperparameter tuning
    <ol>
      <li>Make the parameter a command-line argument</li>
      <li>Make sure outputs don’t clobber each other</li>
      <li>Supply hyperparameters to training job</li>
    </ol>
  </li>
</ul>

<p><br /></p>

<h2 id="regularization-for-sparsity">Regularization for sparsity</h2>

<hr />

<ul>
  <li>
    <p><code class="language-plaintext highlighter-rouge">Zeroing out</code> coefficients can help with performance, especially with large models and sparse inputs</p>

    <ul>
      <li>Fewer coefficients to store / load → Reduce memory, model size</li>
      <li>Fewer multiplications needed → Increase prediction speed</li>
    </ul>

\[L(w, D)+\lambda\sum^n|w|\]

    <ul>
      <li>L2 regularization only makes weights small, not zero</li>
    </ul>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">Feature crosses</code> lead to lots of input nodes, so having zero weights is especially important</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">L0-norm</code>(the count of non-zero weights) is an NP-hard, non-convex optimization problem</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">L1 norm</code>(sum of absolute values of the weights) is convex and efficient; it tends to encourage sparsity in the model</p>
  </li>
  <li>
    <p>There are many possible choices of norms</p>
  </li>
</ul>

<p><img src="/img/posts/art-and-science-of-ml/04.png" alt="" class="center-75" /><br />
<span class="caption"></span></p>

<ul>
  <li>
    <p><code class="language-plaintext highlighter-rouge">Elastic nets</code> combine the feature selection of L1 regularization with the generalizability of L2 regularization</p>

\[L(w,D)+\lambda_1\sum^n|w|+\lambda_2\sum^nw^2\]
  </li>
</ul>

<p><br /></p>

<h2 id="logistic-regression">Logistic Regression</h2>

<hr />

<ul>
  <li>Transform linear regression by a sigmoid activation function</li>
</ul>

<p><img src="/img/posts/art-and-science-of-ml/05.png" alt="Logistic Regression" class="center-75" /><br />
<span class="caption">Logistic Regression</span></p>

<ul>
  <li>
    <p>The output of Logistic Regression is a calibrated probability estimate</p>

    <ul>
      <li>Useful because we can cast <code class="language-plaintext highlighter-rouge">binary classification</code> problems into <code class="language-plaintext highlighter-rouge">probabilistic</code> problems: <strong>Will customer buy item?</strong> becomes <strong>Predict the probability that customer buys item</strong></li>
    </ul>
  </li>
  <li>
    <p>Typically, use <code class="language-plaintext highlighter-rouge">cross-entropy</code> (related to Shannon’n information theory) as the error metric</p>

    <ul>
      <li>Less emphasis on errors where the output is relatively close to the label.</li>
    </ul>

\[LogLoss = \sum_{(x,y)\in D}-ylog(\hat{y})-(1-y)log(1-\hat{y})\]
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">Regularization</code> is important in logistic regression because driving the loss to zero is difficult and dangerous</p>

    <ul>
      <li>Weights will be driven to -inf and +inf the longer we train</li>
      <li>Near the asymptotes, gradient is really small</li>
    </ul>
  </li>
  <li>
    <p>Often we do both <code class="language-plaintext highlighter-rouge">regularization</code> and <code class="language-plaintext highlighter-rouge">early stopping</code> to counteract overfitting</p>
  </li>
</ul>

<p><img src="/img/posts/art-and-science-of-ml/06.png" alt="" class="center-75" /><br />
<span class="caption"></span></p>

<ul>
  <li>In many real-world problems, the probability is not enough; we need to make a <code class="language-plaintext highlighter-rouge">binary decision</code>
    <ul>
      <li>Choice of <code class="language-plaintext highlighter-rouge">threshold</code> is important and can be tuned</li>
    </ul>
  </li>
  <li>Use the <code class="language-plaintext highlighter-rouge">ROC curve</code> to choose the decision threshold based on decision criteria</li>
</ul>

<p><img src="/img/posts/art-and-science-of-ml/07.png" alt="" class="center-75" /><br />
<span class="caption"></span></p>

<ul>
  <li>The <code class="language-plaintext highlighter-rouge">Area-Under-Curve(AUC)</code> provides an aggregate measure of performance across all possible classification thresholds
    <ul>
      <li>AUC helps you choose between models when you don’t know what decision threshold is going to be ultimately used.</li>
      <li>“If we pick a random positive and a random negative, what’s the probability my model scores them in the correct relative order?”</li>
    </ul>
  </li>
  <li>Logistic Regression predictions should be <code class="language-plaintext highlighter-rouge">unbiased</code>
    <ul>
      <li><strong>average of predictions == average of observations</strong></li>
      <li>Look for bias in slices of data. this can guide improvements</li>
    </ul>
  </li>
  <li>Use calibration plots of bucketed bias to find slices where your model performs poorly</li>
</ul>

<p><img src="/img/posts/art-and-science-of-ml/08.png" alt="" class="center-75" /><br />
<span class="caption"></span></p>

<p><br /></p>

<h2 id="neural-networks">Neural Networks</h2>

<hr />

<ul>
  <li>Feature crosses help linear models work in nonlinear problems
    <ul>
      <li>But there tends to be a limit…</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">Combine features</code> as an alternative to feature crossing
    <ul>
      <li>Structure the model so that features are combined Then the combinations may be combined</li>
      <li>How to choose the combinations? Get the model to learn them</li>
    </ul>
  </li>
  <li>A Linear Model can be represented as nodes and edges</li>
  <li>Adding a Non-Linearity</li>
</ul>

<p><img src="/img/posts/art-and-science-of-ml/09.png" alt="" class="center-75" /><br />
<span class="caption"></span></p>

<p><img src="/img/posts/art-and-science-of-ml/10.png" alt="" class="center-75" /><br />
<span class="caption"></span></p>

<ul>
  <li>Our favorite non-linearity is the <code class="language-plaintext highlighter-rouge">Rectified Linear Unit</code> (ReLU)</li>
</ul>

\[f(x) = max(0,x)\]

<ul>
  <li>
    <p>There are many different ReLU variants</p>

\[Softplus = ln(1+e^x)\]

\[Leaky ReLU=f(x)=\begin{cases}0.01x&amp;for&amp;x&gt;0\\x&amp;for&amp;x\le0\end{cases}\]

\[PReLU=f(x)=\begin{cases}\alpha x&amp;for&amp;x&gt;0\\x&amp;for&amp;x\le0\end{cases}\]

\[ReLU6=min(max(0,x),6)\]

\[ELU=f(x)=\begin{cases}\alpha (e^x-1)&amp;for&amp;x&gt;0\\x&amp;for&amp;x\le0\end{cases}\]
  </li>
  <li>
    <p>Neural Nets can be <code class="language-plaintext highlighter-rouge">arbitrarily complex</code></p>

    <ul>
      <li>Hidden layer - Training done via BackProp algorithm: gradient descent in very non-convex space</li>
      <li>To increase <code class="language-plaintext highlighter-rouge">hidden dimension</code>, I can add <code class="language-plaintext highlighter-rouge">neurons</code></li>
      <li>To increase <code class="language-plaintext highlighter-rouge">function composition</code>, I can add <code class="language-plaintext highlighter-rouge">layers</code></li>
      <li>To increase <code class="language-plaintext highlighter-rouge">multiple labels per example</code>, I can add <code class="language-plaintext highlighter-rouge">outputs</code></li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h2 id="training-neural-networks">Training Neural Networks</h2>

<hr />

<ul>
  <li>DNNRegressor usage is similar to LinearRegressor</li>
</ul>

<figure class="highlight"><pre><code class="language-python" data-lang="python"> 
<span class="n">myopt</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">estimator</span><span class="p">.</span><span class="n">DNNRegressor</span><span class="p">(</span><span class="n">model_dir</span><span class="o">=</span><span class="n">outdir</span><span class="p">,</span>
<span class="err">​</span>					<span class="n">hidden_units</span><span class="o">=</span><span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">20</span><span class="p">],</span>
<span class="err">​</span>					<span class="n">feature_colimns</span><span class="o">=</span><span class="n">INPUT_COLS</span><span class="p">,</span>
<span class="err">​</span>					<span class="n">optimizer</span><span class="o">=</span><span class="n">myopt</span><span class="p">,</span>
<span class="err">​</span>					<span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="n">NSTEPS</span> <span class="o">=</span> <span class="p">(</span><span class="mi">100</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">traindf</span><span class="p">))</span> <span class="o">/</span> <span class="n">BATCH_SIZE</span>
<span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">(</span><span class="n">input_fn</span><span class="o">=</span><span class="n">train_input_fn</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="n">NSTEPS</span><span class="p">)</span></code></pre></figure>

<ul>
  <li>Use <code class="language-plaintext highlighter-rouge">momentum-based</code> optimizers e.g. Adagrad(the default) or Adam.</li>
  <li><code class="language-plaintext highlighter-rouge">Specify number</code> of hidden nodes.</li>
  <li>Optionally, can also regularize using <code class="language-plaintext highlighter-rouge">dropout</code></li>
  <li>Three common failure modes for gradient descent</li>
</ul>

<p><img src="/img/posts/art-and-science-of-ml/11.png" alt="" class="center-75" /><br />
<span class="caption"></span></p>

<ul>
  <li>There are benefits if feature values are <code class="language-plaintext highlighter-rouge">small</code> numbers
    <ul>
      <li>Roughly zero-centered, [-1, 1] range often works well</li>
      <li>Small magnitudes help gradient descent <code class="language-plaintext highlighter-rouge">converge</code> and avoid NaN trop</li>
      <li><code class="language-plaintext highlighter-rouge">Avoiding outlier</code> values helps with generalization</li>
    </ul>
  </li>
  <li>We can use standard methods to make feature values <code class="language-plaintext highlighter-rouge">scale to small numbers</code>
    <ul>
      <li>Linear scaling</li>
      <li>Hard cap (clipping) to max, min</li>
      <li>Log scaling</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">Dropout</code> layers are a form of regularization</li>
</ul>

<p><img src="/img/posts/art-and-science-of-ml/12.png" alt="" class="center-75" /><br />
<span class="caption"></span></p>

<ul>
  <li>Dropout simulates <code class="language-plaintext highlighter-rouge">ensemble</code> learning</li>
  <li>Typical values for dropout are between <code class="language-plaintext highlighter-rouge">20 to 50</code> percent</li>
  <li>The <code class="language-plaintext highlighter-rouge">more</code> drop out, the <code class="language-plaintext highlighter-rouge">stronger</code> the regularization
    <ul>
      <li>0.0 = no dropout regularization</li>
      <li>Intermediate values more useful, a value of dropout=0.2 is typical</li>
      <li>1.0 = drop everything out! learns nothing</li>
    </ul>
  </li>
  <li>Dropout acts as another form of <code class="language-plaintext highlighter-rouge">Regularization</code>. It forces data to flow down <code class="language-plaintext highlighter-rouge">multiple</code> paths so that there is a more even spread. It also simulates <code class="language-plaintext highlighter-rouge">ensemble</code> learning. Don’t forget to scale the dropout activations by the inverse of the <code class="language-plaintext highlighter-rouge">keep probability</code>. We remove dropout during <code class="language-plaintext highlighter-rouge">inference</code>.</li>
</ul>

<p><br /></p>

<h2 id="multi-class-neural-networks">Multi-class Neural Networks</h2>

<hr />

<ul>
  <li>Logistic regression provides useful probabilities for <code class="language-plaintext highlighter-rouge">binary-class</code> problems</li>
  <li>There are lots of <code class="language-plaintext highlighter-rouge">multi-class</code> problems
    <ul>
      <li>How do we extend the logits idea to multi-class classifiers?</li>
    </ul>
  </li>
  <li>Idea: Use separate output nodes for each possible class</li>
  <li>Add additional constraint, that total outputs = 1.0</li>
</ul>

<p><img src="/img/posts/art-and-science-of-ml/13.png" alt="" class="center-75" /><br />
<span class="caption"></span></p>

<ul>
  <li>Use one <code class="language-plaintext highlighter-rouge">softmax</code> loss for all possible classes</li>
</ul>

<figure class="highlight"><pre><code class="language-python" data-lang="python"> 
<span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">matmul</span><span class="p">(...)</span> <span class="c1"># logits for each output node -&gt; shape=[batch_size, num_classes]
</span><span class="n">labels</span> <span class="o">=</span>                <span class="c1"># one-hot encoding in [0, num_class] -&gt; shape=[batch_size, num_classes]
</span><span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">reduce_mean</span><span class="p">(</span>
<span class="err">​</span>			<span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">softmax_cross_entropy_with_logits_v2</span><span class="p">(</span>
<span class="err">​</span>				<span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="c1"># shape=[batch_size]
</span><span class="p">}</span></code></pre></figure>

<ul>
  <li>Use sotfmax only when classes are <code class="language-plaintext highlighter-rouge">mutually exclusive</code>
    <ul>
      <li>“Multi-Class, Single_label Classification”</li>
      <li>An example may be a member of only one class.</li>
      <li>Are there multi-class setting where examples may belong to more than one class?</li>
    </ul>
  </li>
</ul>

<figure class="highlight"><pre><code class="language-python" data-lang="python"> 
<span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">sigmoid_cross_entropy_with_logits</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="c1"># shape=[batch_size, num_classes]</span></code></pre></figure>

<ul>
  <li>If you have hundreds or thousands of classes, loss computation can become a significant <code class="language-plaintext highlighter-rouge">bottleneck</code>
    <ul>
      <li>Need to evaluate every output node for every example</li>
    </ul>
  </li>
  <li>Approximate versions of softmax exist
    <ul>
      <li><strong>Candidate Sampling</strong> calculates for all the positive labels, but only for a random sample of negatives: <code class="language-plaintext highlighter-rouge">tf.nn.sampled_softmax_loss</code></li>
      <li><strong>Noise-contrastive</strong> approximates the denominator of softmax by modeling the distribution of outputs: <code class="language-plaintext highlighter-rouge">tf.nn.nce_loss</code></li>
    </ul>
  </li>
  <li>For our classification output, if we have both <strong>mutually exclusive labels and probabilities</strong>, we should use <code class="language-plaintext highlighter-rouge">tf.nn.softmax_cross_entropy_with_logits_v2</code>.</li>
  <li>If the labels are <strong>mutually exclusive</strong>, but the <strong>probabilities aren’t</strong>, we should use <code class="language-plaintext highlighter-rouge">tf.nn.sparse_softmax_cross_entropy_with_logits</code>.</li>
  <li>If our labels <strong>aren’t mutually exclusive</strong>, we should use <code class="language-plaintext highlighter-rouge">tf.nn.sigmoid_cross_entropy_with_logits</code>.</li>
</ul>

  </section>
</div>

<div id="top" class="top-btn" onclick="moveTop()">
  <i class="fas fa-chevron-up"></i>
</div>

<script>
  var lastScrollTop = 0;
  window.onscroll = function () {
    var st = document.body.scrollTop || document.documentElement.scrollTop;
    if (st > 250) {
      document.getElementById("top").style.display = "block"
      if (st > lastScrollTop) {
        document.getElementById("top").style.opacity = 0
      } else {
        document.getElementById("top").style.opacity = 1
      }
    } else {
      document.getElementById("top").style.opacity = 0
      if (st > lastScrollTop) {
        document.getElementById("top").style.display = "none"
      }
    }
    lastScrollTop = st <= 0 ? 0 : st;
  }
  function moveTop() {
    document.body.scrollTop = 0
    document.documentElement.scrollTop = 0
  }
</script>

<!-- Footer -->
<footer>
  <div class="footer">
    Copyright © 2022
    <a href="https://cyc1am3n.github.io">Daeyoung Kim</a>.
  </div>
</footer>

  </div>
</body>

</html>