<!DOCTYPE html>
<html lang="en">
<head>
  <!-- <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script> -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  
  <!-- Favicon code from realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#8b51a3">
<meta name="msapplication-TileColor" content="#563d7c">
<meta name="theme-color" content="#ffffff">

  <!--jQuery-->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

  <!-- Fonts & Icons -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
  <link href='//spoqa.github.io/spoqa-han-sans/css/SpoqaHanSans-kr.css' rel='stylesheet' type='text/css'>

  <!-- CSS -->
  <link rel="stylesheet" href="/assets/css/main.css">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>[CS231n] Introduction to Neural Networks | Daeyoung Kim</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="[CS231n] Introduction to Neural Networks" />
<meta name="author" content="cyc1am3n" />
<meta property="og:locale" content="en_GB" />
<meta name="description" content="이 포스트는 Stanford University cs231n의 4번째 강의인 Introduction to Neural Networks의 강의노트입니다." />
<meta property="og:description" content="이 포스트는 Stanford University cs231n의 4번째 강의인 Introduction to Neural Networks의 강의노트입니다." />
<link rel="canonical" href="http://localhost:4000/2018/10/28/introduction-to-nn.html" />
<meta property="og:url" content="http://localhost:4000/2018/10/28/introduction-to-nn.html" />
<meta property="og:site_name" content="Daeyoung Kim" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-10-28T00:50:54+09:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="[CS231n] Introduction to Neural Networks" />
<meta name="twitter:site" content="@daeyoung__k" />
<meta name="twitter:creator" content="@cyc1am3n" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"cyc1am3n"},"dateModified":"2018-10-28T00:50:54+09:00","datePublished":"2018-10-28T00:50:54+09:00","description":"이 포스트는 Stanford University cs231n의 4번째 강의인 Introduction to Neural Networks의 강의노트입니다.","headline":"[CS231n] Introduction to Neural Networks","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2018/10/28/introduction-to-nn.html"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/assets/img/smile.png"},"name":"cyc1am3n"},"url":"http://localhost:4000/2018/10/28/introduction-to-nn.html"}</script>
<!-- End Jekyll SEO tag -->

</head>
<!--jQuery-->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
<body>
  <div class="container">
    

<section id="header-nav">
  <header>
    <nav>
      <ul>
        
        <!-- others -->
        <a href="/">
          <li class="btn-nav">Home</li>
        </a>
        
        
        
          <a href="/blog">
            <li class="current btn-nav">Blog</li>
          </a>
          <a href="/tags">
            <li class="btn-nav">Tags</li>
          </a>
        
        
      </ul>
    </nav>
  </header>
</section>
<div id="post">
  <section class="post-header">
    <h1 class="title">[CS231n] Introduction to Neural Networks</h1>
    <p class="subtitle">Lecture 4 Note</p>
    <p class="meta">
      October 28, 2018
    </p>
  </section>
  <section class="post-content">
    <p><em>이 포스트는 Stanford University <a href="http://cs231n.stanford.edu/2017/">cs231n</a>의 4번째 강의인 Introduction to Neural Networks의 강의노트입니다.</em></p>

<p><br /></p>

<h2 id="back-propagation">Back Propagation</h2>

<hr />

<p>시작에 앞서 computational graph를 활용한 아주 간단한 예제를 살펴보자.</p>

\[f(x,y,z)=(x+y)z\]

<p><img src="/assets/img/posts/cs231n/lec04-01.png" alt="" class="center-75" /><br />
<span class="caption"></span></p>

<p>우리가 구하고 싶은 것은 각 input에 대한 f의 gradient이다.(경사 하강법을 적용해야하기 때문에) 이는 최종 출력 노드에서 얻을 수 있는 gradient로 부터 하나씩 거꾸로 거슬러 올라가면서 얻을 수 있을 것이다.</p>

<p>일단 \(q\)를 \(x + y\) 라고 정의를 하면, \(x\) 와 \(y\) 에 대한 \(q\) 의 gradient는 다음과 같다.</p>

\[q=x+y\qquad{\partial q \over \partial x} = 1,\ {\partial q \over \partial y} = 1\]

<p>이제 \(f\) 를 \(qz\) 라고 놓으면, \(q\) 와 \(z\) 에 대한 \(f\) 의 gradient도 구할 수 있다.</p>

\[f=qz\qquad{\partial f \over \partial q} = z,\ {\partial f \over \partial z} = q\]

<p>곱셈에 대한 미분은 곱셈규칙에 의해서 서로 곱한 값이 미분값이 되기 때문이다.</p>

<p>그리고 이제 우리가 구해야 하는 것은</p>

\[{\partial f \over \partial x},\ {\partial f \over \partial y},\ {\partial f \over \partial z}\]

<p>이고, 위에서 얻은 값을 <strong>Chain Rule</strong>을 통해서 구하면 된다.</p>

<p>위의 그림과 같이, \(x=-2,\ y=5,\ z=-4\) 라고 놓고 계산을 해보자.</p>

<p>먼저, \({\partial f \over\partial f}\) 는 당연하게 1이다.</p>

<p>이제 뒤로 이동해서 \({\partial f \over \partial q},\ {\partial f \over \partial z}\) 를 구하면 그 값은 \(z\) 와 \(q\) 이므로</p>

\[{\partial f \over \partial q} = z=-4,\ {\partial f \over \partial z} = q=x+y=3\]

<p>이 될 것이다.</p>

<p>그리고 한 번 더 뒤로 이동을 하면 우리가 구하려고 했던 \({\partial f \over \partial x},\ {\partial f \over \partial y},\ {\partial f \over \partial z}\) 또한 구할 수 있다.</p>

<p><strong>Chain Rule</strong> 에 의해서,</p>

\[\begin{matrix}
{\partial f \over \partial x}= {\partial f \over \partial q}\cdot {\partial q \over \partial x}=(-4)\cdot1=-4,\\
{\partial f \over \partial y}={\partial f \over \partial q}\cdot {\partial q \over \partial y}=(-4)\cdot1=-4,
\\ {\partial f \over \partial z}=3
\end{matrix}\]

<p>이 될 것이다.</p>

<p>이것은 \(x\) 와 \(y\) 를 조금 바꿨을 때 그것이 최종 출력 값인 \(f\) 에 대해 -4 만큼의 영향력을 가지고, \(z\) 는 3만큼의 영향력을 미치는 것을 의미한다.</p>

<p>위와 같이 입력 값이 출력 값에 미치는 영향을 얻기 위해 각 노드에서 가질 수 있는 <strong>local gradient</strong> 값과 최종 출력 값에서 부터 나오는 <strong>gradient</strong> 를 곱하는 과정을 뒤에서부터 반복하고, 이를 <strong>Back Propagation</strong> 이라고 한다.</p>

<p>이 과정은 뒤에서 공부할 <strong>neural network</strong> 에서 중요하게 쓰이는데, 복잡한 layer들이 여러 겹 이어져서 하나의 모델을 이룬다고 하면 각 input과 그 가중치에 대한 gradient를 위와 같은 과정을 통해 쉽게(?) 구할 수 있게 되어 이를 통해 효율적인 학습을 할 수 있게 된다.</p>

<p>한편 back propagation에서 더 알아야 할  몇 가지의 내용이 있다.</p>

<p>먼저 여러 연산 게이트에 대한 backward flow 패턴인데, 간략히 설명하자면</p>

<ul>
  <li>
    <p><strong>add</strong> gate는 뒤에서부터 오는 gradient 를 똑같이 나눠주고(gradient distributor),</p>
  </li>
  <li>
    <p><strong>max</strong> gate는 입력 값이 더 큰 값에만 gradient를 나눠주고 그렇지 않는 값에는 0을 주는 라우터의 역할을 하며(gradient router),</p>
  </li>
  <li>
    <p><strong>mul</strong> gate는 각 입력 값의 gradient를 교환해준다(gradient switcher 혹은 scaler).</p>
  </li>
</ul>

<p>또한 아래와 같이 하나의 노드가 2개 이상의 branch를 가질 때는 각 branch에서 얻어지는 gradient를 더해야 한다.</p>

<p><img src="/assets/img/posts/cs231n/lec04-02.png" alt="" class="center-95" /><br />
<span class="caption"></span></p>

<p>마지막으로 스칼라가 아닌 벡터를 입력으로 가지는 연산의 gradient를 생각해보자.</p>

<p>모든 흐름은 정확하게 같은데, 차이점이라고 한다면 gradient는 다변수 벡터 함수의 도함수행렬인 <a href="https://ko.wikipedia.org/wiki/%EC%95%BC%EC%BD%94%EB%B9%84_%ED%96%89%EB%A0%AC">Jacobian 행렬 (링크)</a>이 될 것이다.</p>

<p>아래 예제를 통해서 생각해보자.</p>

<p><img src="/assets/img/posts/cs231n/lec04-03.png" alt="" class="center-95" /><br />
<span class="caption"></span></p>

<p>여기서의 입력은 4096 차원의 벡터이고, 이는 CNN에서 흔하게 볼 수 있는 입력 사이즈이다.</p>

<p>또한 이 노드는 요소별로 최대값을 취하며 \((f(x)=max(0,x))\), 출력 또한 4096 차원의 벡터이다.</p>

<p>여기서 gradient인 Jacobian 행렬의 사이즈는 4096 * 4096 이고, 여기에서 100개를 동시에 입력으로 받는 미니 배치를 사용하면 Jacobian은 409600*409600까지 커질 수가 있다.</p>

<p>이는 너무 커서 실용적이지는 못하고, 사실 이 전체 Jacobian을 구할 필요는 없다.</p>

<p>실제로 벡터의 각 요소가 출력 값에 미치는 영향을 얻어야 하기 때문에 Jacobian의 <strong>대각행렬</strong>만 알면 된다.</p>

<p>그리고 이 gradient는 항상 변수와 같은 모양(shape)을 가지고 있어 계산을 한 후에 변수의 모양과 같은지 확인하는 식으로 제대로 gradient를 구했는 지를 검사할 수 있다.</p>

<p>지금까지 각 노드를 local하게 보았고, upstream gradient와 같이 local gradient를 계산을 해봤는데, 이것을 forward, backward pass의 API로 생각할 수 있다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ComputationalGraph</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
	<span class="c1">#...
</span>	<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
		<span class="c1"># 1. [pass inputs to input gates...]
</span>		<span class="c1"># 2. forward the computational graph:
</span>		<span class="k">for</span> <span class="n">gate</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">graph</span><span class="p">.</span><span class="n">nodes_topologically_sorted</span><span class="p">():</span>
			<span class="n">gate</span><span class="p">.</span><span class="n">forward</span><span class="p">()</span>
		<span class="k">return</span> <span class="n">loss</span> <span class="c1"># the final gate in the graph outputs the loss
</span>	<span class="k">def</span> <span class="nf">backward</span><span class="p">():</span>
		<span class="k">for</span> <span class="n">gate</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">graph_nodes_topologically_sorted</span><span class="p">()):</span>
			<span class="n">gate</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1"># little piece of backprop (chain rule applied)
</span>		<span class="k">return</span> <span class="n">inputs_gradients</span>
</code></pre></div></div>

<p>한편 foward pass에서 중요한 부분은 여기서 계산한 값들을 저장해야 한다는 것이다. 이 값들은 backward pass에서 사용되기 때문이다.</p>

<p>지금까지 했던 내용을 요약하자면…</p>

<ul>
  <li>nueral nets은 매우 크기가 크다: 모든 매개변수에 대해 gradient를 일일히 계산하는 것은 실용적이지 못하다.</li>
  <li><strong>backpropagation</strong> = 모든 입력 / 매개변수 / 계산에 필요한 중간 변수 의 gradient를 계산하기 위해 computational graph를 따라서 chain rule을 이용해 재귀적으로 적용함</li>
  <li>실제 구현은 <strong>forward</strong> / <strong>backward</strong> API를 구현하는 그래프 구조를 통해 이루어진다.</li>
  <li><strong>forward</strong>: 연산의 결과를 계산하고 메모리에 gradient 계산에 필요한 중간 변수들을 저장한다.</li>
  <li><strong>backward</strong>: chain rule을 적용해서 입력에 대한 loss function의 gradient를 계산한다.</li>
</ul>

<p><br /></p>

<h2 id="neural-network">Neural Network</h2>

<hr />

<p>사람들은 신경망(Neural Network)과 뇌 사이에서 많은 유추와 여러 가지의 생물학적인 영감을 이끌어낸다.</p>

<p>일단은 그것에 대해 이야기 하는 것은 뒤로 미루고, 식으로 표현된 신경망에 대해 이야기를 해보자.</p>

<p>다음과 같은 아주 간단한 형태의 2겹의 레이어를 가진 신경망을 한 번 살펴보자.</p>

\[f=W_2max(0,W_1x)\]

<p><img src="/assets/img/posts/cs231n/lec04-04.png" alt="" class="center-95" /><br />
<span class="caption"></span></p>

<p>위 신경망은 첫 번째로 \(W_1\)과 \(x\)의 행렬 곱을 중간 변수로 가지고 \(max(0, W)\) 라는 <strong>비선형 함수</strong>를 이용해서 선형 값을 얻는다.</p>

<p>선형 레이어들만 계속 쌓는다면 결국 하나의 선형 함수로 표현이 가능하기 때문에 여기서 비선형 함수가 중요한 역할을 한다. (꼭 \(max(0,W)\)일 필요는 없고, 추후에 배울 여러 비선형 함수 또한 사용 가능하다.)</p>

<p>또한 신경망을 폭 넓게 표현하자면 함수들의 집합(class)이라고 할 수 있다.</p>

<p>이는 신경망이 비선형의 복잡한 함수를 만들기 위해서 간단한 함수들을 계층적으로 여러 겹 쌓아 올린 것이기 때문이다.</p>

<p><br /></p>

<p>한편, 아까 잠깐 언급했던 신경망과 생물학적인 연결을 다시 이어보자.</p>

<p>사실 이 관계는 매우 loose한데, 이러한 연결과 영감이 어디에서 왔는지를 이해할 필요는 있다.</p>

<p>생물학적인 관점에서 많은 뉴런들이 수상돌기에 이어져 서로 연결되어 존재하고, 그 사이에서 각 뉴런을 따라 전달되는 신호가 있다는 것을 알 수 있다.</p>

<p>또한 여러 수상돌기를 통해서 얻어진 신호를 세포체(Cell body)에서 종합하고, 이를 하류 뉴런과 연결된 다른 세포체로 전달을 한다.</p>

<p>지금까지 해왔던 것을 살펴본다면, 각 computational node와 뉴런의 동작이 다음과 같이 비슷한 방식으로 이루어 진다는 것을 알 수 있다.</p>

<p><img src="/assets/img/posts/cs231n/lec04-05.png" alt="" class="center-95" /><br />
<span class="caption"></span></p>

<p>또한 왼쪽 하단과 같이 활성 함수(activation function)을 볼 수 있는데, 이것은 입력을 받아 나중에 출력이 될 하나의 숫자를 보여주는 것이다.</p>

<p>사실 실제 생물학적 뉴런은 이것보다 훨씬 복잡한데, 위와 같은 loose한 연결을 할 때는 조심할 필요가 있다.</p>

<p>마지막으로 신경망의 feed-forward 계산의 예를 살펴보고 마무리를 하면 될 것 같다.</p>

<p><img src="/assets/img/posts/cs231n/lec04-06.png" alt="" class="center-95" /><br />
<span class="caption"></span></p>

<p>여기서 가지고 있는 벡터 행렬의 출력은 비선형성을 가진다.</p>

<p>여기서 사용한 활성 함수 \(f\) 는 sigmoid이고, 입력으로 \(x\) 벡터를 받아 \(W_1\)과 곱해서(\(h_1\)) 비선형성을 적용한 다음 두 번째 히든 레이어인 \(h_2\)를 얻기 위해서 \(W_2\)와 행렬곱을 적용하고 한 번 더 비선형성을 적용한 후에 최종 출력을 얻어낸다.</p>

<p>그리고 앞에서 보았던 backward pass는 신경망을 학습시키기 위해서 필요한데, 그냥 backpropagation을 적용하기만 하면 된다.</p>

<p><br /></p>

<p>지금까지 했던 부분을 요약하자면,</p>

<ul>
  <li>뉴런을 선형 레이어와 fully-connected로 재배열했고,</li>
  <li>레이어의 추상화는 모든 것을 계산하는데 매우 효율적인 벡터화된 코드를 사용할 수 있게 하는 좋은 속성을 가지고 있으며,</li>
  <li>신경망이 생물학적 비유와 loose한 영감을 가지고 있지만, 실제로는 뉴런과는 많이 다르다는 것을 간과하면 안된다.</li>
</ul>

<p>라는 것이다.</p>

<p>다음 시간에는 <strong>Convolutional Neural Networks</strong>를 알아볼 것이다.</p>

  </section>
</div>

<div id="top" class="top-btn" onclick="moveTop()">
  <i class="fas fa-chevron-up"></i>
</div>

<script>
  var lastScrollTop = 0;
  window.onscroll = function () {
    var st = document.body.scrollTop || document.documentElement.scrollTop;
    if (st > 250) {
      document.getElementById("top").style.display = "block"
      if (st > lastScrollTop) {
        document.getElementById("top").style.opacity = 0
      } else {
        document.getElementById("top").style.opacity = 1
      }
    } else {
      document.getElementById("top").style.opacity = 0
      if (st > lastScrollTop) {
        document.getElementById("top").style.display = "none"
      }
    }
    lastScrollTop = st <= 0 ? 0 : st;
  }
  function moveTop() {
    document.body.scrollTop = 0
    document.documentElement.scrollTop = 0
  }
</script>

<!-- Footer -->
<footer>
  <div class="footer">
    Copyright © 2022
    <a href="https://cyc1am3n.github.io">Daeyoung Kim</a>.
  </div>
</footer>

  </div>
</body>

</html>