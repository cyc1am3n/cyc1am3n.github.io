<!DOCTYPE html>
<html lang="en">
<head>
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  
  <!-- Favicon code from realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#8b51a3">
<meta name="msapplication-TileColor" content="#563d7c">
<meta name="theme-color" content="#ffffff">

  <!--jQuery-->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

  <!-- Fonts & Icons -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
  <link href='//spoqa.github.io/spoqa-han-sans/css/SpoqaHanSans-kr.css' rel='stylesheet' type='text/css'>

  <!-- CSS -->
  <link rel="stylesheet" href="/assets/css/main.css">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>[Paper] Pix2Pix Review | Daeyoung Kim</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="[Paper] Pix2Pix Review" />
<meta name="author" content="cyc1am3n" />
<meta property="og:locale" content="en_GB" />
<meta name="description" content="제목: Image-to-Image Translation with Conditional Adversarial Networks 저자: Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros @ Berkeley AI Research (BAIR) Laboratory, UC Berkeley 학회/년도: CVPR 2017" />
<meta property="og:description" content="제목: Image-to-Image Translation with Conditional Adversarial Networks 저자: Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros @ Berkeley AI Research (BAIR) Laboratory, UC Berkeley 학회/년도: CVPR 2017" />
<link rel="canonical" href="http://localhost:4000/2020/03/01/pix2pix.html" />
<meta property="og:url" content="http://localhost:4000/2020/03/01/pix2pix.html" />
<meta property="og:site_name" content="Daeyoung Kim" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-03-01T22:20:00+09:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="[Paper] Pix2Pix Review" />
<meta name="twitter:site" content="@daeyoung__k" />
<meta name="twitter:creator" content="@cyc1am3n" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"cyc1am3n"},"dateModified":"2020-03-01T22:20:00+09:00","datePublished":"2020-03-01T22:20:00+09:00","description":"제목: Image-to-Image Translation with Conditional Adversarial Networks 저자: Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros @ Berkeley AI Research (BAIR) Laboratory, UC Berkeley 학회/년도: CVPR 2017","headline":"[Paper] Pix2Pix Review","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2020/03/01/pix2pix.html"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/assets/img/smile.png"},"name":"cyc1am3n"},"url":"http://localhost:4000/2020/03/01/pix2pix.html"}</script>
<!-- End Jekyll SEO tag -->

</head>
<!--jQuery-->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
<body>
  <div class="container">
    

<section id="header-nav">
  <header>
    <nav>
      <ul>
        
        <!-- others -->
        <a href="/">
          <li class="btn-nav">Home</li>
        </a>
        
        
        
          <a href="/blog">
            <li class="current btn-nav">Blog</li>
          </a>
          <a href="/tags">
            <li class="btn-nav">Tags</li>
          </a>
        
        
      </ul>
    </nav>
  </header>
</section>
<div id="post">
  <section class="post-header">
    <h1 class="title">[Paper] Pix2Pix Review</h1>
    <p class="subtitle">Image-to-Image Translation with Conditional Adversarial Networks</p>
    <p class="meta">
      March 1, 2020
    </p>
  </section>
  <section class="post-content">
    <blockquote>
  <p><strong>제목</strong>: <strong>Image-to-Image Translation with Conditional Adversarial Networks</strong></p>

  <p><strong>저자</strong>: <em>Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros</em> @ Berkeley AI Research (BAIR) Laboratory, UC Berkeley</p>

  <p><strong>학회/년도</strong>: CVPR 2017</p>
</blockquote>

<hr />

<h2 id="abstract">Abstract</h2>

<ul>
  <li>input ↔ output image의 mapping과 이 <strong>mapping을 학습시키기 위한 loss-function</strong>을 배우는 네트워크</li>
</ul>

<hr />

<h2 id="introduction">Introduction</h2>

<ul>
  <li>image-to-image translation task를 정의</li>
  <li>CNN을 학습시키기에 효과적인 loss function을 디자인 하는 것에는 많은 수고가 필요하다.
    <ul>
      <li>ex) 단순하게 GT와 생성된 image 간의 Euclidean distance로 loss를 잡으면 blurry 한 결과가 나온다.</li>
    </ul>
  </li>
  <li>그 대신에 “실제와 구별하기 힘든 output을 만들자”라는 high-level goal을 가지고 GAN에 적용해본다면?
    <ul>
      <li>위 예시처럼 blurry image는 fake라고 인식해서 이런 이미지는 안 만들어질 것이다.</li>
      <li>GAN은 data에 맞춰진 loss를 통해 학습하고, 원래 같았으면 loss를 각각 만들어야했을 여러 task에서 사용할 수 있다.</li>
    </ul>
  </li>
  <li>이 논문에서는 conditional GAN을 활용해서 image-to-image translation의 다양한 문제를 해결하는 모델을 제시할 것이다. (게다가 간단하기까지함)</li>
  <li>코드는 <a href="https://github.com/phillipi/pix2pix">여기</a>서 볼 수 있다.</li>
</ul>

<hr />

<h2 id="related-work">Related work</h2>

<h3 id="structured-losses-for-image-modeling">Structured losses for image modeling</h3>

<ul>
  <li>unstructured loss와 structured loss의 차이는?</li>
  <li>conditional GAN 은 structured loss를 학습하고 output과 target 간의 다른 구조에 페널티를 준다.</li>
</ul>

<h3 id="conditional-gans">Conditional GANs</h3>

<ul>
  <li>
    <p>GAN을 conditional setting에서 사용한 것은 이 논문이 처음이 아니며 다양한 분야(future frame prediction, image generation, 심지어 image-to-image mapping 까지도)에서 시도되었다.</p>
  </li>
  <li>이 논문이 제시하는 방법은 한정된 application에서만 쓸 수 있는 것이 아니며 다른 것들 보다 간단하다.</li>
  <li>이전 방법과 generator와 discriminator의 네트워크 아키텍쳐 부분에서 다르다. (다시 살펴볼 예정)
    <ul>
      <li>generator로 <strong>U-Net</strong> 기반 아키텍쳐를 골라서 원본 이미지의 정보 손실을 최소화시켰다.</li>
      <li>discriminator로 <strong>Patch GAN</strong> 이라는 아키텍쳐를 골라서 디테일한 feature까지 잡아낼 수 있다.</li>
    </ul>
  </li>
</ul>

<hr />

<h2 id="method">Method</h2>

<p><img src="https://user-images.githubusercontent.com/11629647/75626368-70caf900-5c0a-11ea-9a4a-a0a5057cb802.png" alt="" class="center-75" /><br />
<span class="caption"></span></p>

<ul>
  <li>(vanilla) GAN은 random noise \(z\) 에서 output image \(y\)를 만들어낸다. \(G:z\rightarrow y\)</li>
  <li>conditional GAN은 source image \(x\)와 \(z\) 에서 \(y\) 를 만들어낸다. \(G:\{x,z\}\rightarrow y\)
    <ul>
      <li>이때 discriminator에 넣을 input에도 \(x\)를 같이 넣어준다. (Figure 2 참고, 그리고 사실 \(z\) 안씀)</li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h3 id="objective">Objective</h3>

<ul>
  <li>conditional GAN의 loss 는 아래 식과 같이 표현할 수 있다.</li>
</ul>

\[\mathcal{L}_{cGAN}(G, D)=\mathbb{E}_{x,y}[\log D(x, y)]+\mathbb{E}_{x,z}[\log(1-D(x,G(x,z)))] \tag{1}\]

<ul>
  <li>discriminator 에 condition을 주지 않으면 어떻게 되는지 비교하기 위해서 다음 loss를 가지고도 실험할 것이다.</li>
</ul>

\[\mathcal{L}_{GAN}(G, D)=\mathbb{E}_{y}[\log D(y)]+\mathbb{E}_{x,z}[\log(1-D(G(x,z)))] \tag{2}\]

<ul>
  <li>
    <p>또한 고전적인 접근법을 참고해서 generator가 생성한 image와 target image의 차이를 줄이는 GAN loss를 추가하면 성능이 좋아진다.</p>

    <ul>
      <li>이때 L2보다 L1을 사용하면 덜 blur한 이미지가 만들어진다.
\(\mathcal{L}_{L1}(G)=\mathbb{E}_{x,y,z}[||y-G(x,z)||_1]\tag{3}\)</li>
    </ul>
  </li>
  <li>
    <p>이를 다 합쳐서 objective를 만들면 다음과 같다.</p>
  </li>
</ul>

\[G^*=\text{arg }\underset{G}{\text{min}}\ \underset{D}{\text{max}}\ \mathcal{L}_{cGAN}(G,D)+\lambda \mathcal{L}_{L1}(G)\tag{4}\]

<ul>
  <li>generator에 가우시안 noise \(z\) 를 같이 입력해봤는데, 그다지 효과적이지는 않았다.(오히려 discriminator가 noise를 무시함)
    <ul>
      <li>그래서 이 논문의 모델에서는 generator의 입력에 noise \(z\)​를 넣지 않고, 학습 및 평가시에 드롭 아웃 형태의 노이즈를 넣어주는 식으로 전환함.</li>
      <li>하지만 stochasticity가 떨어지는 모습을 발견했고, 이 문제를 해결하는 conditional GAN을 설계하는 것은 future work으로..</li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h3 id="network-architectures">Network architectures</h3>

<ul>
  <li>generator와 discriminator에는 Conv-BatchNorm-ReLu 로 구성된 모듈들을 사용했다.</li>
</ul>

<h4 id="generator-with-skips">Generator with skips</h4>

<ul>
  <li>image-to-image translation의 어려운 점은 고해상도 입력-출력 매핑이라는 점이다.
    <ul>
      <li>그래서 기존의 encoder-decoder 네트워크를 사용한다면 encoder 통과시 downsample 되면서 정보의 손실이 발생할 수 있게 된다.</li>
    </ul>
  </li>
  <li>이를 보완해 low-level information을 input-output가 skip connection을 통해 공유하는 형태인 U-Net 을 사용하자.
    <ul>
      <li>총 \(n\)개의 layer가 있다고 한다면, \(i\) 번째 layer와 \(n-i\) 번째 layer를 연결한다.</li>
      <li>skip connection은 간단히 channel을 concatenate하는 식으로 만들었다.</li>
    </ul>
  </li>
</ul>

<p><img src="https://user-images.githubusercontent.com/11629647/75626371-74f71680-5c0a-11ea-8501-9f9a5d13d7fb.png" alt="" class="center-75" /><br />
<span class="caption"></span></p>

<h4 id="markovian-discriminator-patchgan">Markovian discriminator (PatchGAN)</h4>

<ul>
  <li>L1이나 L2 loss를 쓰면 blur하게 high frequency 정보를 잘 잡아낼 수는 없지만 low frequency 정보는 잘 잡아낸다.</li>
  <li>이 점을 적용해 GAN discriminator 가 high frequency를 잡아내고 L1이 low frequency를 잡아내도록 하자. (식 4 참고)</li>
  <li>discriminator가 high frequency를 잡도록 방향을 잡아 이미지의 일부분에 집중해서 local patch를 보고 구분하도록 하는 PatchGAN을 설계했다.
    <ul>
      <li>이미지를 \(N\times N\) 개의 patch로 나눠 각 patch가 real인지 fake 구분하도록 만들었다.</li>
      <li>\(N^2\) 개의 output을 averaging해서 최종 output으로 사용했다.</li>
    </ul>
  </li>
  <li>patch를 많이 만들면 파라미터도 적게 들고 빠르고 다양한 크기의 이미지에도 적용할 수 있다. (퀄리티도 좋음)</li>
  <li>PatchGAN discriminator는 패치 크기 이상으로 분리된 픽셀 사이의 독립성을 가정해 이미지를 Markov random field로 모델링한다.
    <ul>
      <li>이를 통해 PatchGAN이 texture/style 을 이해할 수 있게 되는 것이다.</li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h3 id="optimization-and-inference">Optimization and inference</h3>

<ul>
  <li>Adam
    <ul>
      <li>lr = 0.0002, \(\beta_1\) = 0.5, \(\beta_2\) = 0.999</li>
    </ul>
  </li>
  <li>test 시에도 dropout을 적용하며, batch norm을 쓸 때도 test data에서 얻은 통계를 사용한다.</li>
  <li>batch가 1인 경우에는 instance normalization이 되며 이는 image generation task 에서 효과적인 결과를 낸다.
    <ul>
      <li>batch size를 1 ~ 10 까지 두고 실험을 해보았다.</li>
    </ul>
  </li>
</ul>

<hr />

<h2 id="experiment">Experiment</h2>

<h3 id="analysis-of-the-objective-function">Analysis of the objective function</h3>

<p><img src="https://user-images.githubusercontent.com/11629647/75626421-cbfceb80-5c0a-11ea-849b-39be21f53ef8.png" alt="" class="center-95" /><br />
<span class="caption"></span></p>

<ul>
  <li>
    <p>위 그림은 식 \((4)\) 에서 각 loss의 component를 가지고  label → photo 문제에 대한 결과를 낸 것이다.</p>
  </li>
  <li>
    <p>여기에 대해 FCN score를 내보면 다음과 같다.</p>

    <p><img src="https://user-images.githubusercontent.com/11629647/75626428-e0d97f00-5c0a-11ea-87ee-2f5303038aeb.png" alt="" class="center-75" /><br />
<span class="caption"></span></p>
  </li>
  <li>
    <p>L1 loss는 edge의 위치를 정확하게 특정하기 힘들어 blur 한 이미지를 만들기 때문에 average인 grayish color가 나오게 된다.</p>
  </li>
  <li>
    <p>반면에 adversarial loss(GAN loss)에서는 grayish color는 realistic 하지 않다고 판단하기에 좀더 true color distribution에 가까운 이미지를 만들어내도록 한다. (그래서 cGAN loss만 쓰는게 더 colorful 해 보임)</p>
  </li>
</ul>

<h3 id="analysis-of-the-generator-architecture">Analysis of the generator architecture</h3>

<p><img src="https://user-images.githubusercontent.com/11629647/75626430-e59e3300-5c0a-11ea-9c10-bbbb2a90eca1.png" alt="" class="center-75" /><br />
<span class="caption"></span></p>

<p><img src="https://user-images.githubusercontent.com/11629647/75626437-ec2caa80-5c0a-11ea-9162-427cfc3a6cd2.png" alt="" class="center-75" /><br />
<span class="caption"></span></p>

<ul>
  <li>generator의 아키텍쳐로 Encoder-Decoder를 쓰기에는 적합하지 않다.(오히려 cGAN loss를 쓰면 떨어짐)</li>
  <li>U-Net을 써야 성능이 오른다.</li>
</ul>

<h3 id="from-pixelgans-to-patchgans-to-imagegans">From PixelGANs to PatchGANs to ImageGANs</h3>

<ul>
  <li>다양한 크기의 patch를 discriminator 에 넣어 실험을 진행해보았다.
    <ul>
      <li>\(1\times 1\) 이 PixelGAN이고, 이미지 원본 크기인 \(286\times286\)를 ImageGAN이라고 했다.</li>
    </ul>
  </li>
</ul>

<p><img src="https://user-images.githubusercontent.com/11629647/75626459-15e5d180-5c0b-11ea-863d-4659292eda8b.png" alt="" class="center-95" /><br />
<span class="caption"></span></p>

<p><img src="https://user-images.githubusercontent.com/11629647/75626461-18e0c200-5c0b-11ea-9f44-23d68fbf4f79.png" alt="" class="center-75" /><br />
<span class="caption"></span></p>

<ul>
  <li>PixelGAN이나 ImageGAN보다는 PatchGAN의 성능이 잘나오더라.
    <ul>
      <li>PixelGAN은 spatial 정보를 잡을 수 없고 ImageGAN은 파라미터와 depth가 더 많이 필요해 학습시키기가 어렵다.</li>
    </ul>
  </li>
</ul>

<h3 id="semantic-segmentation">Semantic Segmentation</h3>

<ul>
  <li>Semantic Segmentation은 output이 input보다 덜 복잡한 task이다.</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/11629647/75626463-1d0cdf80-5c0b-11ea-98c9-c4520714af67.png" alt="" class="center-95" /><br />
<span class="caption"></span></p>

<p><img src="https://user-images.githubusercontent.com/11629647/75626464-2007d000-5c0b-11ea-8e7d-8a985fe0ba9d.png" alt="" class="center-75" /><br />
<span class="caption"></span></p>

<ul>
  <li>그래서 그런지 cGAN loss를 써도 잘 하긴 하지만 L1 loss만 있어도 잘 하더라.</li>
</ul>

  </section>
</div>

<div id="top" class="top-btn" onclick="moveTop()">
  <i class="fas fa-chevron-up"></i>
</div>

<script>
  var lastScrollTop = 0;
  window.onscroll = function () {
    var st = document.body.scrollTop || document.documentElement.scrollTop;
    if (st > 250) {
      document.getElementById("top").style.display = "block"
      if (st > lastScrollTop) {
        document.getElementById("top").style.opacity = 0
      } else {
        document.getElementById("top").style.opacity = 1
      }
    } else {
      document.getElementById("top").style.opacity = 0
      if (st > lastScrollTop) {
        document.getElementById("top").style.display = "none"
      }
    }
    lastScrollTop = st <= 0 ? 0 : st;
  }
  function moveTop() {
    document.body.scrollTop = 0
    document.documentElement.scrollTop = 0
  }
</script>

<!-- Footer -->
<footer>
  <div class="footer">
    Copyright © 2022
    <a href="https://cyc1am3n.github.io">Daeyoung Kim</a>.
  </div>
</footer>

  </div>
</body>

</html>